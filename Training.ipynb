{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/rpal/Drive_10TB/John/Control Drop\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7961/4214021680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtractorNetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObjectTactileEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dropping_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBerrettHandGym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "import torch as th\n",
    "from src.RL.Networks.ExtractorNetworks import ObjectTactileEncoder\n",
    "from src.RL.control_dropping_env import BerrettHandGym\n",
    "\n",
    "DATA_SAVE_PATH = os.path.join(os.getcwd(), '..', 'Data_Collection')\n",
    "\n",
    "\n",
    "env = BerrettHandGym(detailed_training=True)\n",
    "state_space = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    \n",
    "    # Extract data from each data point and create numpy arrays\n",
    "    palm_tactile_list = []\n",
    "    finger_1_tactile_list = []\n",
    "    finger_2_tactile_list = []\n",
    "    finger_3_tactile_list = []\n",
    "    palm_location_list = []\n",
    "    finger_1_location_list = []\n",
    "    finger_2_location_list = []\n",
    "    finger_3_location_list = []\n",
    "    obj_location_list = []\n",
    "    obj_velocity_list = []\n",
    "    action_list = []\n",
    "    new_object_location_list = []\n",
    "    \n",
    "    for data_point in data:\n",
    "        state, obj_location_vector = data_point\n",
    "        palm_tactile_list.append(state['palm_tactile'])\n",
    "        finger_1_tactile_list.append(state['finger_1_tactile'])\n",
    "        finger_2_tactile_list.append(state['finger_2_tactile'])\n",
    "        finger_3_tactile_list.append(state['finger_3_tactile'])\n",
    "        palm_location_list.append(state['palm_location'])\n",
    "        finger_1_location_list.append(state['finger_1_location'])\n",
    "        finger_2_location_list.append(state['finger_2_location'])\n",
    "        finger_3_location_list.append(state['finger_3_location'])\n",
    "        obj_location_list.append(state['obj_location'].flatten())\n",
    "        obj_velocity_list.append(state['obj_velocity'].flatten())\n",
    "        action_list.append(state['action'])\n",
    "        new_object_location_list.append(obj_location_vector.flatten())\n",
    "\n",
    "    # Create a single numpy array for each component\n",
    "    palm_tactile_array = np.array(palm_tactile_list).squeeze(axis=1)\n",
    "    finger_1_tactile_array = np.array(finger_1_tactile_list).squeeze(axis=1)\n",
    "    finger_2_tactile_array = np.array(finger_2_tactile_list).squeeze(axis=1)\n",
    "    finger_3_tactile_array = np.array(finger_3_tactile_list).squeeze(axis=1)\n",
    "    palm_location_array = np.array(palm_location_list)\n",
    "    finger_1_location_array = np.array(finger_1_location_list)\n",
    "    finger_2_location_array = np.array(finger_2_location_list)\n",
    "    finger_3_location_array = np.array(finger_3_location_list)\n",
    "    obj_location_array = np.array(obj_location_list)\n",
    "    obj_velocity_array = np.array(obj_velocity_list)\n",
    "    action_array = np.array(action_list)\n",
    "    new_object_location_array = np.array(new_object_location_list)\n",
    "\n",
    "    # Concate all arrays along axis 1\n",
    "    data_array = np.concatenate([\n",
    "        palm_tactile_array, finger_1_tactile_array, finger_2_tactile_array, finger_3_tactile_array,\n",
    "        palm_location_array, finger_1_location_array, finger_2_location_array, finger_3_location_array,\n",
    "        obj_location_array, obj_velocity_array, action_array, new_object_location_array\n",
    "    ], axis=1)\n",
    "\n",
    "    # Save the data array to file\n",
    "    if os.path.exists(os.path.join(file_path, 'Actions_data.npy')):\n",
    "        old_data = np.load(os.path.jostart_lr= 0.003\n",
    "end_lr = 0.0001\n",
    "factor = 0.999\n",
    "def lr_schedule():\n",
    "    global start_lr, end_lr, factor\n",
    "    ret_lr = start_lr\n",
    "    start_lr *= factor\n",
    "    return ret_lrshape), mult(state_space['finger_1_tactile'].shape), mult(state_space['finger_2_tactile'].shape), mult(state_space['finger_3_tactile'].shape), mult(state_space['palm_location'].shape), mult(state_space['finger_1_location'].shape), mult(state_space['finger_2_location'].shape), mult(state_space['finger_3_location'].shape), mult(state_space['obj_location'].shape), mult(state_space['obj_velocity'].shape), 5, ]\n",
    "\n",
    "    indexes = np.zeros(len(sizes), dtype=int)\n",
    "    for i in range(len(sizes)):\n",
    "        indexes[i] = sizes[i] + indexes[i-1] if i > 0 else sizes[i]\n",
    "    palm_tactile_array, finger_1_tactile_array, finger_2_tactile_array, finger_3_tactile_array, \\\n",
    "    palm_location_array, finger_1_location_array, finger_2_location_array, finger_3_location_array, \\\n",
    "    obj_location_array, obj_velocity_array, action_array, new_object_location_array = np.split(data_array, indexes, axis=1)\n",
    "\n",
    "    # Create a list of data points from the components\n",
    "    data = []\n",
    "    num_samples = data_array.shape[0]\n",
    "    for i in range(num_samples):\n",
    "        state = {\n",
    "            'palm_tactile': palm_tactile_array[i].reshape((1, -1)),\n",
    "            'finger_1_tactile': finger_1_tactile_array[i].reshape((1, -1)),\n",
    "            'finger_2_tactile': finger_2_tactile_array[i].reshape((1, -1)),\n",
    "            'finger_3_tactile': finger_3_tactile_array[i].reshape((1, -1)),\n",
    "            'palm_location': palm_location_array[i],\n",
    "            'finger_1_location': finger_1_location_array[i],\n",
    "            'finger_2_location': finger_2_location_array[i],\n",
    "            'finger_3_location': finger_3_location_array[i],\n",
    "            'obj_location': obj_location_array[i].reshape((state_space['obj_location'].shape[0], state_space['obj_location'].shape[1])),\n",
    "            'obj_velocity': obj_velocity_array[i].reshape((state_space['obj_velocity'].shape[0], state_space['obj_velocity'].shape[1])),\n",
    "            'action': action_array[i],\n",
    "        }\n",
    "        obj_location_vector = new_object_location_array[i]\n",
    "        data.append((state, obj_location_vector))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(os.path.join(os.getcwd(), 'Data_Collection', 'Action_Samples', 'Actions_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Collection\n",
    "\n",
    "def DataCollect(num_steps):\n",
    "    done = False\n",
    "    data = []\n",
    "    data_point = env.reset()[0].copy()\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        if done:\n",
    "            save_data(data, os.path.join(os.getcwd(), 'Data_Collection', 'Action_Samples'))\n",
    "            data = []\n",
    "            data_point = env.reset()[0].copy()\n",
    "        i+=1\n",
    "        action = env.action_space.sample()\n",
    "        print('Action:', action)\n",
    "        data_point['action'] = action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ball_locations = env.simController.get_object_locations()\n",
    "        print('Ball locations:', ball_locations)\n",
    "        data.append((data_point, np.array(ball_locations)))\n",
    "        data_point = state.copy()\n",
    "    \n",
    "DataCollect(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr= 0.003\n",
    "end_lr = 0.0001\n",
    "factor = 0.999\n",
    "def lr_schedule():\n",
    "    global start_lr, end_lr, factor\n",
    "    ret_lr = start_lr\n",
    "    start_lr *= factor\n",
    "    return ret_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Keyword Model!!\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "def train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    batch_size = 16, \n",
    "    epochs = 1000, \n",
    "    learning_rate = 5e-3, \n",
    "    log_interval = 50, \n",
    "    no_cuda = False, \n",
    "    seed = 1, \n",
    "    is_lstm=False,\n",
    "    patience = 10):\n",
    "\n",
    "  model.set_batch_size(batch_size)\n",
    "  print(model._batch_size)\n",
    "  use_cuda = not no_cuda and th.cuda.is_available()\n",
    "  device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  print(device)\n",
    "  kwargs = {}\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  noise_level = 0.001  \n",
    "\n",
    "  def train(model, device, train_loader, optimizer, is_lstm=is_lstm):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    for data, target in train_loader:\n",
    "        th.cuda.empty_cache()\n",
    "        i+=1\n",
    "        if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "        # print('data', data.shape)\n",
    "        data = data.to(device).float()\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_target = target.float()\n",
    "        output_prediction = model(data)\n",
    "        _, target_indices = output_target.max(dim=2)\n",
    "        loss = 0\n",
    "        for t in range(output_prediction.size(1)):\n",
    "            loss += criterion(output_prediction[:, t], target_indices[:, t]) * batch_size\n",
    "        loss /= output_prediction.size(1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule()\n",
    "        total_loss+=loss.item()\n",
    "        if i % log_interval == 0:\n",
    "            try:\n",
    "                print(f'Avg Loss: {(total_loss/i+1)}%')\n",
    "            except:\n",
    "                pass\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "  def validation(model, device, val_loader, is_lstm=is_lstm):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    with th.no_grad():\n",
    "      for data, target in val_loader:\n",
    "        if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "        data = data.to(device).float()\n",
    "        target = target.to(device)\n",
    "        output_target = target.float()\n",
    "        output_prediction = model(data)\n",
    "        _, target_indices = output_target.max(dim=2)\n",
    "        val_loss = 0\n",
    "        for t in range(output_prediction.size(1)):\n",
    "            val_loss += criterion(output_prediction[:, t], target_indices[:, t]) * batch_size\n",
    "        val_loss /= output_prediction.size(1)\n",
    "        loss_total += val_loss.item()\n",
    "\n",
    "    val_loss = loss_total / len(val_loader.dataset)\n",
    "    print('Validation_loss:', val_loss)\n",
    "    return val_loss\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr = start_lr,\n",
    "        eps=1e-7,\n",
    "        weight_decay=0.005,\n",
    "        # momentum=0.92,\n",
    "        # centered=True\n",
    "    )\n",
    "  print('Training...')\n",
    "  for epoch in range(1, epochs+1):\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    if epoch % 10 == 0 :\n",
    "        val_loss = validation(model, device, val_loader)\n",
    "    if epoch % 50 == 0:\n",
    "        model.save_checkpoint(os.path.join(os.getcwd(),'..','..', 'data', 'Models', 'KeywordModel', 'Training', 'Checkpoints', f'LSTMKeywordCheckpoint_{epoch}.zip'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes For Training the Dynamics Mode:\n",
    "\n",
    "### Data\n",
    " - `Data_Collection/Action_Samples_1`: a non-temporal based dataset of size > 100K. Used for training the Spatial Encoder Model `ObjectTactileEncoder_Additive`\n",
    "\n",
    " - `Data_Collection/Time_Dependent_Action_Samples_1`: a temporal-based dataset of size > N. Created with a Temporal Context `T_buffer` = 3. Used for training the Temporal-Spatial Encoder Model \n",
    " `TemporalObjectTactileEncoder_Additive`\n",
    "\n",
    "  - `Data_Collection/Time_dependent_Action_Samples_2`: a temporal-based dataset of size > N. Created with a Temporal Context `T_buffer` = 3. Used for training the Temporal-Spatial Encoder Model \n",
    "\n",
    "   - `Data_Collection/Action_Pred_Time_dependent_Action_Samples_1`: a temporal-based dataset of size > N. Created with a Temporal Context `T_buffer` = 5. Used for training the Action Prediction Model and Temporal-Spatial Encoder Model \n",
    "\n",
    " Note: the `Additive` part of the model's name refers to how encodings are colliegated, i.e. we add Positional and Value encodings as opposed to Concatenation. The Concatenation based model was not as useful as the additive model\n",
    "\n",
    "#### What is in the Dataset?\n",
    "\n",
    "\n",
    "\n",
    "##### Inputs:\n",
    "- **palm_tactile**: \n",
    "    Shape: (3, 24) where `3=T_buffer` and `24` is the Tactile sensor reading vector.\n",
    "\n",
    "- **finger_1_tactile**:\n",
    "    Shape: (3, 24) where `3=T_buffer` and `24` is the Tactile sensor reading vector.\n",
    "\n",
    "- **finger_2_tactile**: \n",
    "    Shape: (3, 24) where `3=T_buffer` and `24` is the Tactile sensor reading vector.\n",
    "\n",
    "- **finger_3_tactile**: \n",
    "    Shape: (3, 24) where `3=T_buffer` and `24` is the Tactile sensor reading vector.\n",
    "\n",
    "- **palm_location**: \n",
    "    Shape: (3, 74) where `3=T_buffer` and `74` contains the XYZ positions for all tactile sensors.\n",
    "\n",
    "- **finger_1_location**:\n",
    "    Shape: (3, 74) where `3=T_buffer` and `104` contains the XYZ positions for all tactile sensors.\n",
    "\n",
    "- **finger_2_location**: \n",
    "    Shape: (3, 74) where `3=T_buffer` and `104` contains the XYZ positions for all tactile sensors.\n",
    "\n",
    "- **finger_3_location**: \n",
    "    Shape: (3, 74) where `3=T_buffer` and `104` contains the XYZ positions for all tactile sensors.\n",
    "\n",
    "- **obj_location**:\n",
    " - Shape: (3, 7, 6) where `3=T_buffer`, `7=OBJECT_QUANTITY` and `6=XYZ_RollYawPitch` for each of the balls in the scene.\n",
    "\n",
    "- **obj_velocity**: \n",
    "    Shape: (3, 7, 6) where `3=T_buffer`, `7=OBJECT_QUANTITY` and `6=XYZ_RollYawPitch_Velocity` for each of the balls in the scene.\n",
    "\n",
    "- **state_attrib**: \n",
    "    Shape: (45,) ... As of right now these are not being used\n",
    "\n",
    "- **action**: \n",
    "    Shape: (5,) Action input for Spread, F1, F2, F3, ... the last index isn't being used, it used to be for scaling. \n",
    "\n",
    "##### Outputs:\n",
    "\n",
    "- **obj_location**: \n",
    "    Shape: (42,), \n",
    "\n",
    "- **finger_1_location**: \n",
    "    Shape: (27,)\n",
    "\n",
    "- **finger_2_location**: \n",
    "    Shape: (27,)\n",
    "\n",
    "- **finger_3_location**: \n",
    "    Shape: (27,)\n",
    "\n",
    "- **palm_location**: \n",
    "    Shape: (27,)\n",
    "\n",
    "- **finger_1_tactile**: \n",
    "    Shape: (9,)\n",
    "\n",
    "- **finger_2_tactile**: \n",
    "    Shape: (9,)\n",
    "\n",
    "- **finger_3_tactile**: \n",
    "    Shape: (9,)\n",
    "\n",
    "- **palm_tactile**: \n",
    "    Shape: (9,)\n",
    "\n",
    "- **hand_config**: \n",
    "    Shape: (7,)\n",
    "\n",
    "- **obj_count**: \n",
    "    Shape: (5,)\n",
    "\n",
    "- **progress_bar**: \n",
    "    Shape: (1,)\n",
    "\n",
    "- **reward**: \n",
    "    Shape: (1,)\n",
    "\n",
    "\n",
    "## Model Architecture:\n",
    "- Transformer Encoder for learning Spatial (and Temporal) representations of our data\n",
    "    a. We Encode based on Type of Data (Object Vs. Limb) and Type of Projection (Temporal, Spatial, Value). \n",
    "    b. We use Sinosodial Embeddings for all Temporal encodings\n",
    "    c. We concat our Projected tensors into an object Matrix where the input to the transformer will be `(BATCH_SIZE, NUM_OBJECTS*TIME_DIMENSION + 1, EMBEDDING_DIMENSION)`. The `+ 1` is captured as the output Vector and is the last object inserted into the transformer. This embedding is initialized to all zeros.\n",
    "    d. We Mask our input with a probability `p=0.11`, randomly applying zero mask to our inputs. Play with the masking mechanism, as first iterations demonstrate the maksing mechanism is a promising feature.\n",
    "    e. We return Output[-1], ie. the last embedding vector from the transformer. This is the zero input vector we created previously. The goal is to learn a good embedding vector based on the K transformer blocks we use to output this vector to the rest of the pipeline, whether its the RL or Pretraining process. \n",
    "\n",
    "- Residual Network for updating the Encoding, we condition the Encoding of the Observation with an Action Vector. We then perform a Gating between the Old Encoding and the New Encoding. This has not been tested, experiment with Gating Mechanism and Removal.\n",
    "\n",
    "- We then branch out into K different Networks, where each sub-network recieves the State Encoding conditioned on the Action, and uses this to predict the K feature of the Future State\n",
    "\n",
    "- Note: We only use raw data sensor data and object data to perform our predictions. We do not use any hand crafted or additional values in our prediction. Feel free to create a new encoding for other values, and throw them into the Transformer. The Transformer will use whatever you give it to make its predictions better.\n",
    "\n",
    "\n",
    "## TODO:\n",
    "- Action Prediction Model (inspired by Tedrake): Given `S[T], S[T+1]` predict `A[T]`\n",
    "- reinforcment learning: Collect pretrained dynamix metrics.\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization:: Use env stbl3 or raylib\n",
    "\n",
    "import os\n",
    "CONTROL_DROP_DIR = os.environ[\"CONTROL_DROP_DIR\"]\n",
    "\n",
    "import sys\n",
    "\n",
    "# os.chdir(\"..\")\n",
    "import torch as th\n",
    "from gymnasium.spaces import Box\n",
    "from control_dropping_rpal.RL.control_dropping_env import BerrettHandGym, T_buffer\n",
    "\n",
    "from math import inf, radians, degrees\n",
    "from stable_baselines3 import PPO, A2C\n",
    "\n",
    "DATA_SAVE_PATH = os.path.join(CONTROL_DROP_DIR, \"Data_Collection\")\n",
    "MODEL_PATH = os.path.join(CONTROL_DROP_DIR, \"control_dropping/src/RL/Training/Checkpoints/TransformerFeatureEncoder/Expert_rl_5000_steps.zip\")\n",
    "\n",
    "# model = PPO.load(MODEL_PATH)\n",
    "# model.set_env(env)\n",
    "state_space = {\n",
    "    \"palm_tactile\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            24,\n",
    "        ),\n",
    "    ),  # Value\n",
    "    \"finger_1_tactile\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            24,\n",
    "        ),\n",
    "    ),  # Value\n",
    "    \"finger_2_tactile\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            24,\n",
    "        ),\n",
    "    ),  # Value\n",
    "    \"finger_3_tactile\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            24,\n",
    "        ),\n",
    "    ),  # Value\n",
    "    # 'tactile_pos': Box(low= -inf, high= inf, shape=(378, )), # Position\n",
    "    \"finger_1_location\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            2 + 34 * 3,\n",
    "        ),\n",
    "    ),  # Joint pos [Theta_1, Theta_2] + [xyz*34]\n",
    "    \"finger_2_location\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            2 + 34 * 3,\n",
    "        ),\n",
    "    ),\n",
    "    \"finger_3_location\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            2 + 34 * 3,\n",
    "        ),\n",
    "    ),\n",
    "    \"palm_location\": Box(\n",
    "        low=-inf,\n",
    "        high=inf,\n",
    "        shape=(\n",
    "            T_buffer,\n",
    "            2 + 24 * 3,\n",
    "        ),\n",
    "    ),\n",
    "    \"obj_location\": Box(low=-inf, high=inf, shape=(T_buffer, 7, 6)),  # Position\n",
    "    \"obj_velocity\": Box(\n",
    "        low=-inf, high=inf, shape=(T_buffer, 7, 6)\n",
    "    ),  # Value, Concat with angular velocity\n",
    "    \"state_attrib\": Box(\n",
    "        low=-inf, high=inf, shape=(45,)\n",
    "    ),  # Ball Cnt, Progress, Prev.Actions, hand_cfg, hand_trq (44)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Create example data\n",
    "def create_example_data():\n",
    "    # Create two dictionaries with lists of numpy arrays\n",
    "    data1 = {\n",
    "        \"key1\": [\n",
    "            np.array([1, 2, 3]),\n",
    "            np.array([4, 5, 6]),\n",
    "            np.array([7, 8]),\n",
    "        ],  # Note the last array has a different shape\n",
    "        \"key2\": [\n",
    "            np.array([[1, 2], [3, 4]]),\n",
    "            np.array([[5, 6], [7, 8]]),\n",
    "            np.array([9, 10]),\n",
    "        ],  # Note the last array has a different shape\n",
    "    }\n",
    "\n",
    "    data2 = {\n",
    "        \"key1\": [\n",
    "            np.array([10, 20, 30]),\n",
    "            np.array([40, 50, 60]),\n",
    "            np.array([70, 80, 90]),\n",
    "        ],\n",
    "        \"key2\": [\n",
    "            np.array([[10, 20], [30, 40]]),\n",
    "            np.array([[50, 60], [70, 80]]),\n",
    "            np.array([[90, 100], [110, 120]]),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return [data1, data2]\n",
    "\n",
    "\n",
    "# 2. Implement function to remove inhomogeneous elements\n",
    "def remove_inhomogeneous_elements(aggregated_data):\n",
    "    removal_idxs = set()\n",
    "\n",
    "    # Detect inhomogeneous elements\n",
    "    for data in aggregated_data:\n",
    "        for key, arr_list in data.items():\n",
    "            shapes = [arr.shape for arr in arr_list]\n",
    "            most_common_shape = max(set(shapes), key=shapes.count)\n",
    "\n",
    "            for idx, shape in enumerate(shapes):\n",
    "                if shape != most_common_shape:\n",
    "                    removal_idxs.add(idx)\n",
    "\n",
    "    # Remove inhomogeneous elements\n",
    "    for data in aggregated_data:\n",
    "        for key in data:\n",
    "            data[key] = [\n",
    "                arr for idx, arr in enumerate(data[key]) if idx not in removal_idxs\n",
    "            ]\n",
    "\n",
    "    return aggregated_data, list(removal_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data stuff\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "GAMMA = 0.5\n",
    "\n",
    "\n",
    "def mult(arr, idx=0):\n",
    "    if idx == len(arr):\n",
    "        return 1\n",
    "    return arr[idx] * mult(arr, idx + 1)\n",
    "\n",
    "\n",
    "def save_data(\n",
    "    data,\n",
    "    file_path,\n",
    "):\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path, exist_ok=True)\n",
    "\n",
    "    # Extract data from each data point and create numpy arrays\n",
    "    state_list = [state[0].copy() for state in data]\n",
    "    pred_state_list = [state[1].copy() for state in data]\n",
    "\n",
    "    chunk_data = {\n",
    "        \"states\": state_list,\n",
    "        \"pred_states\": pred_state_list,\n",
    "    }\n",
    "    import re\n",
    "\n",
    "    i = (\n",
    "        sorted([int(re.sub(r\"\\D\", \"\", s)) for s in os.listdir(file_path)] + [-1])[-1]\n",
    "        + 1\n",
    "    )\n",
    "    chunk_file_path = os.path.join(file_path, f\"Actions_data_{i}.pkl\")\n",
    "\n",
    "    with open(chunk_file_path, \"wb\") as f:\n",
    "        pickle.dump(chunk_data, f)\n",
    "    print(\"Saved to\", chunk_file_path)\n",
    "\n",
    "\n",
    "def mult(shape):\n",
    "    \"\"\"Utility function to compute the product of elements in a shape tuple.\"\"\"\n",
    "    product = 1\n",
    "    for dim in shape:\n",
    "        product *= dim\n",
    "    return product\n",
    "\n",
    "\n",
    "def load_data_node_format(file_path, target_range=4):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    # Load data from pickle files into a list of states\n",
    "    states = []\n",
    "    new_keys = {f\"finger_{i}_tactile\": f\"finger_{i+1}_tactile\" for i in range(3)}\n",
    "    new_keys.update(\n",
    "        {f\"finger_{i}_locaction\": f\"finger_{i+1}_location\" for i in range(3)}\n",
    "    )\n",
    "    new_keys.update({\"palm_locaction\": \"palm_location\"})\n",
    "\n",
    "    for f_path in [\n",
    "        f for f in os.listdir(file_path) if \"Actions_data\" in f\n",
    "    ]:  # For each episode\n",
    "        with open(os.path.join(file_path, f_path), \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        temp_states = []\n",
    "        for state, pred in zip(\n",
    "            data[\"states\"], data[\"pred_states\"]\n",
    "        ):  # For each state[t-1], state[t] pair\n",
    "            if not isinstance(pred, dict):\n",
    "                pred = {key: pred[key] for key in pred.dtype.names}\n",
    "            if \"finger_0_tactile\" in pred:\n",
    "                # Update keys:\n",
    "                pred = {new_keys.get(k, k): v for k, v in pred.items()}\n",
    "            temp_states.append((state, pred))\n",
    "\n",
    "        sts = [s[0] for s in temp_states]\n",
    "        preds = [s[1] for s in temp_states]\n",
    "        # for i in range(len(preds) - 2, -1, -1):\n",
    "        #     preds[i][\"reward\"] += GAMMA * preds[i + 1][\"reward\"]\n",
    "\n",
    "        states += [(s, p) for s, p in zip(sts, preds)]\n",
    "\n",
    "    # Aggregate data by key\n",
    "    aggregated_data = [{}, {}]\n",
    "    for d in states:\n",
    "        for key, value in d[0].items():\n",
    "            if key not in aggregated_data[0]:\n",
    "                aggregated_data[0][key] = []\n",
    "            aggregated_data[0][key].append(value)\n",
    "        for key, value in d[1].items():\n",
    "            if key not in aggregated_data[1]:\n",
    "                aggregated_data[1][key] = []\n",
    "            aggregated_data[1][key].append(value)\n",
    "\n",
    "    # Perform surgery: Some values are input wrong\n",
    "    reward_data = []\n",
    "    for i in range(len(aggregated_data[1][\"reward\"])):\n",
    "        reward_data.append(\n",
    "            np.array([aggregated_data[1][\"reward\"][i]])\n",
    "        )  # Convert to list\n",
    "    aggregated_data[1][\"reward\"] = np.array(reward_data)\n",
    "\n",
    "    # Flatten and format:\n",
    "    aggregated_data = [\n",
    "        {\n",
    "            k: [np.array(v).squeeze() for v in value]\n",
    "            for k, value in aggregated_data[0].items()\n",
    "        },\n",
    "        {\n",
    "            k: [np.array(v).flatten() for v in value]\n",
    "            for k, value in aggregated_data[1].items()\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    stacked_values = np.vstack(aggregated_data[0][\"action\"])\n",
    "    stacked_values /= 3.5\n",
    "    aggregated_data[0][\"action\"] = [\n",
    "        a.squeeze() for a in np.split(stacked_values, len(aggregated_data[0][\"action\"]))\n",
    "    ]\n",
    "\n",
    "    # Find inhomogeneous data:\n",
    "    removal_idxs = set()\n",
    "\n",
    "    for data in aggregated_data:\n",
    "        for key, arr_list in data.items():\n",
    "            shapes = [arr.shape for arr in arr_list]\n",
    "            most_common_shape = max(\n",
    "                set(shapes), key=shapes.count\n",
    "            )  # Detect inhomogeneous elements\n",
    "\n",
    "            for idx, shape in enumerate(shapes):\n",
    "                if shape != most_common_shape:\n",
    "                    removal_idxs.add(idx)\n",
    "\n",
    "    # Remove inhomogeneous elements:\n",
    "    for data in aggregated_data:\n",
    "        for key in data:\n",
    "            data[key] = [\n",
    "                arr for idx, arr in enumerate(data[key]) if idx not in removal_idxs\n",
    "            ]\n",
    "\n",
    "    print(\"Inhomogeneous data:\", removal_idxs)\n",
    "\n",
    "    for data in aggregated_data:\n",
    "        for key, arr in data.items():\n",
    "            arr = np.where(np.isnan(arr), 0, arr)\n",
    "            arr = np.where(arr < -(4**2), 0, arr)\n",
    "            arr = np.where(arr > 4**2, 0, arr)\n",
    "            data[key] = arr\n",
    "\n",
    "    print(\"Shape Min Maxs:\\n\")\n",
    "    for key in aggregated_data[0].keys():\n",
    "        print(\n",
    "            f\"{key}: {aggregated_data[0][key][0].shape}, {np.min(aggregated_data[0][key])}, {np.max(aggregated_data[0][key])}\"\n",
    "        )\n",
    "    for key in aggregated_data[1].keys():\n",
    "        print(\n",
    "            f\"{key}: {np.array(aggregated_data[1][key][0]).shape}, {np.min(aggregated_data[1][key])}, {np.max(aggregated_data[1][key])}\"\n",
    "        )\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "\n",
    "def load_data_state_format(file_path, target_range=4):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    # Load data from pickle files into a list of states\n",
    "    states = []\n",
    "    new_keys = {f\"finger_{i}_tactile\": f\"finger_{i+1}_tactile\" for i in range(3)}\n",
    "    new_keys.update(\n",
    "        {f\"finger_{i}_locaction\": f\"finger_{i+1}_location\" for i in range(3)}\n",
    "    )\n",
    "    new_keys.update({\"palm_locaction\": \"palm_location\"})\n",
    "\n",
    "    for f_path in [\n",
    "        f for f in os.listdir(file_path) if \"Actions_data\" in f\n",
    "    ]:  # For each episode\n",
    "        with open(os.path.join(file_path, f_path), \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        temp_states = []\n",
    "        for state, pred in zip(\n",
    "            data[\"states\"], data[\"pred_states\"]\n",
    "        ):  # For each state[t-1], state[t] pair\n",
    "            if not isinstance(pred, dict):\n",
    "                pred = {key: pred[key] for key in pred.dtype.names}\n",
    "            if \"finger_0_tactile\" in pred:\n",
    "                # Update keys:\n",
    "                pred = {new_keys.get(k, k): v for k, v in pred.items()}\n",
    "\n",
    "            # We collect prediction data for an entire state, we are going to restrict the prediction to a T_buffer of 1 so that it encapsulates only S[T] and enables better representations during the forward pass:\n",
    "            for key in [\n",
    "                k\n",
    "                for k in pred.keys()\n",
    "                if k != \"reward\"\n",
    "                and T_buffer in pred[k].shape\n",
    "                and len(pred[k].shape) > 1\n",
    "            ]:\n",
    "                arr = pred[key]\n",
    "                pred[key] = arr[-1:, :]\n",
    "\n",
    "            action = state[\"action\"] if \"action\" in state else pred[\"action\"]\n",
    "            reward = np.array(\n",
    "                [(pred[\"reward\"] if \"reward\" in pred else state[\"reward\"])]\n",
    "            )\n",
    "\n",
    "            state = {k: v for k, v in state.items() if k not in (\"action\", \"reward\")}\n",
    "            n_state = {k: v for k, v in pred.items() if k not in (\"action\", \"reward\")}\n",
    "            pred = {\"action\": action, \"reward\": reward}\n",
    "\n",
    "            temp_states.append((state, n_state, pred))\n",
    "\n",
    "        states += [(s, n_s, p) for s, n_s, p in temp_states]\n",
    "\n",
    "    # Aggregate data by key\n",
    "    aggregated_data = [{}, {}, {}]\n",
    "    for d in states:\n",
    "        # S[T - 1]\n",
    "        for key, value in d[0].items():\n",
    "            if key not in aggregated_data[0]:\n",
    "                aggregated_data[0][key] = []\n",
    "            aggregated_data[0][key].append(value)\n",
    "        # S[T]\n",
    "        for key, value in d[1].items():\n",
    "            if key not in aggregated_data[1]:\n",
    "                aggregated_data[1][key] = []\n",
    "            aggregated_data[1][key].append(value)\n",
    "\n",
    "        # Prediction:\n",
    "        for key, value in d[2].items():\n",
    "            if key not in aggregated_data[2]:\n",
    "                aggregated_data[2][key] = []\n",
    "            aggregated_data[2][key].append(value)\n",
    "\n",
    "    # Flatten and format:\n",
    "    aggregated_data = [\n",
    "        {k: [np.array(v) for v in value] for k, value in aggregated_data[0].items()},\n",
    "        {k: [np.array(v) for v in value] for k, value in aggregated_data[1].items()},\n",
    "        {\n",
    "            k: (\n",
    "                [np.array(v).flatten() for v in value]\n",
    "            )  # if k != \"reward\" else [np.array(v) for v in value])\n",
    "            for k, value in aggregated_data[2].items()\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    stacked_values = np.vstack(aggregated_data[2][\"action\"])\n",
    "    stacked_values *= 1 / 3.5\n",
    "    aggregated_data[2][\"action\"] = [\n",
    "        a.squeeze() for a in np.split(stacked_values, len(aggregated_data[2][\"action\"]))\n",
    "    ]\n",
    "\n",
    "    for data in aggregated_data:\n",
    "        for key, arr in data.items():\n",
    "            arr = np.where(np.isnan(arr), 0, arr)\n",
    "            arr = np.where(arr < -(4**2), 0, arr)\n",
    "            arr = np.where(arr > 4**2, 0, arr)\n",
    "            data[key] = arr\n",
    "\n",
    "    print(\"Shape Min Maxs:\\n\")\n",
    "    for key in aggregated_data[0].keys():\n",
    "        print(\n",
    "            f\"{key}: {aggregated_data[0][key][0].shape}, {np.min(aggregated_data[0][key])}, {np.max(aggregated_data[0][key])}\"\n",
    "        )\n",
    "    print(\"\\n----------\\n\")\n",
    "    for key in aggregated_data[1].keys():\n",
    "        print(\n",
    "            f\"{key}: {np.array(aggregated_data[1][key][0]).shape}, {np.min(aggregated_data[1][key])}, {np.max(aggregated_data[1][key])}\"\n",
    "        )\n",
    "    print(\"\\n----------\\n\")\n",
    "    for key in aggregated_data[2].keys():\n",
    "        print(\n",
    "            f\"{key}: {np.array(aggregated_data[2][key][0]).shape}, {np.min(aggregated_data[2][key])}, {np.max(aggregated_data[2][key])}\"\n",
    "        )\n",
    "\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads the Data\n",
    "path_dynamix = (\n",
    "    os.path.join(CONTROL_DROP_DIR, \"Data_Collection/Time_Dependent_Samples_4/\")\n",
    ")\n",
    "path_action_pred = os.path.join(CONTROL_DROP_DIR, \"Data_Collection/Action_Pred_Time_Dependent_Samples_4/\")\n",
    "\n",
    "# Time_Dependent_Samples_1: Old Reward Function\n",
    "# Time_Dependent_Samples_2: New Reward Function\n",
    "\n",
    "dynamix_data = load_data_node_format(path_dynamix)\n",
    "pred_data = load_data_state_format(path_action_pred)\n",
    "\n",
    "for data in (\n",
    "    dynamix_data,\n",
    "    pred_data,\n",
    "):\n",
    "    print(\n",
    "        \"Dataset Size:\", \"\\n\".join([f\"{k}: {len(data[0][k])}\" for k in data[0].keys()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collection_dir = os.path.join(CONTROL_DROP_DIR, \"Data_Collection\", \"Action_Samples_2\")\n",
    "\n",
    "# Ensure the 'Data_Collection/Action_Samples' directory exists\n",
    "if not os.path.exists(data_collection_dir):\n",
    "    os.makedirs(data_collection_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualLayer1D(nn.Module):\n",
    "    def __init__(self, feature_dim: int, embed_dim=512, dropout_p=0.1):\n",
    "        super(ResidualLayer1D, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc1 = nn.Linear(\n",
    "            feature_dim,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            embed_dim,\n",
    "            feature_dim,\n",
    "        )\n",
    "        self.n = nn.LayerNorm(feature_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.gelu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.gelu(self.fc2(out))\n",
    "        out = self.n(x + out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlocks1D(nn.Module):\n",
    "    def __init__(self, feature_dim: int, num_blocks: int, embed_dim=512, dropout_p=0.1):\n",
    "        super(ResidualBlocks1D, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                ResidualLayer1D(feature_dim, embed_dim, dropout_p=dropout_p)\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_length=16):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.encoding = nn.Embedding(max_length, d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         sequence_length = x.size(1)\n",
    "#         positions = torch.arange(sequence_length, dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "#         return self.encoding(positions)\n",
    "\n",
    "# class TimeSequenceModel(nn.Module):\n",
    "#     def __init__(self, embedding_size, action_size, num_layers, num_heads=8, hidden_size=2048, max_length=16):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.action_size = action_size\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.positional_encoding = PositionalEncoding(embedding_size + action_size, max_length)\n",
    "#         transformer_layer = nn.TransformerEncoderLayer (\n",
    "#             batch_first=True,\n",
    "#             d_model=self.embedding_size,\n",
    "#             nhead=8,\n",
    "#             dim_feedforward=hidden_size,\n",
    "#             dropout=0.08,\n",
    "#         )\n",
    "#         self.transformer = nn.TransformerEncoder(transformer_layer, 8)\n",
    "#         self.fc = nn.Linear(embedding_size + action_size, embedding_size)\n",
    "\n",
    "#     def forward(self, states, actions):\n",
    "#         batch_size, sequence_length, _ = states.shape\n",
    "\n",
    "#         # Concatenate states and actions\n",
    "#         inputs = torch.cat((states, actions), dim=-1)\n",
    "\n",
    "#         # Apply positional encoding\n",
    "#         positions = self.positional_encoding(inputs)\n",
    "#         inputs = inputs + positions\n",
    "\n",
    "#         # Create an upper triangular mask for attention\n",
    "#         mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1).to(states.device)\n",
    "#         mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "#         # Pass the inputs through the Transformer\n",
    "#         outputs = self.transformer(inputs, inputs, mask=mask)\n",
    "\n",
    "#         # Pass the outputs through the final fully connected layer\n",
    "#         predictions = self.fc(outputs)\n",
    "\n",
    "#         return predictions\n",
    "\n",
    "\n",
    "# embedded_states = embedding_model(states)\n",
    "# predictions = transformer_model(embedded_states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_dynamix(data):\n",
    "    # Compute mean and standard deviation for each key\n",
    "    mean_dict = [{}, {}]\n",
    "    std_dict = [{}, {}]\n",
    "\n",
    "    for idx in range(len(mean_dict)):\n",
    "        for key in data[idx].keys():\n",
    "            all_values = np.concatenate(\n",
    "                [np.array(arr).flatten() for arr in data[idx][key]]\n",
    "            )\n",
    "            mean_dict[idx][key] = np.mean(all_values, axis=0)\n",
    "            std_dict[idx][key] = np.std(all_values, axis=0)\n",
    "\n",
    "    # Normalize the data\n",
    "    NON_NORM_KEYS = [\n",
    "        \"obj_count\",\n",
    "        \"progress_bar\",\n",
    "        \"state_attrib\"\n",
    "    ]\n",
    "\n",
    "    normalized_data = [data[0], {}]\n",
    "    for key in data[1].keys():\n",
    "        if key in NON_NORM_KEYS:\n",
    "            normalized_data[1][key] = data[1][key]\n",
    "            continue\n",
    "        normalized_data[1][key] = []\n",
    "        for arr in data[1][key]:\n",
    "            normalized_arr = (arr - mean_dict[1][key]) / std_dict[1][key]\n",
    "            normalized_data[1][key].append(normalized_arr)\n",
    "\n",
    "    print(\"Dynamix:\")\n",
    "    print(\"MEAN:\", mean_dict)\n",
    "    print(\"STD:\", std_dict)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def normalize_data_critiq(data):\n",
    "    # Compute mean and standard deviation for each key\n",
    "    mean_dict = [{}, {}, {}]\n",
    "    std_dict = [{}, {}, {}]\n",
    "\n",
    "    for idx in range(len(mean_dict)):\n",
    "        for key in data[idx].keys():\n",
    "            all_values = np.concatenate(\n",
    "                [np.array(arr).flatten() for arr in data[idx][key]]\n",
    "            )\n",
    "            mean_dict[idx][key] = np.mean(all_values, axis=0)\n",
    "            std_dict[idx][key] = np.std(all_values, axis=0)\n",
    "\n",
    "    # Normalize the data\n",
    "    NON_NORM_KEYS = [\"action\", \"state_attrib\"]\n",
    "\n",
    "    normalized_data = [data[0], data[1], {}]\n",
    "    for key in data[2].keys():\n",
    "        if key in NON_NORM_KEYS:\n",
    "            normalized_data[2][key] = data[2][key]\n",
    "            continue\n",
    "        normalized_data[2][key] = []\n",
    "        for arr in data[2][key]:\n",
    "            normalized_arr = (arr - mean_dict[2][key]) / std_dict[2][key]\n",
    "            normalized_data[2][key].append(normalized_arr)\n",
    "\n",
    "    print(\"Critiq:\")\n",
    "    print(\"MEAN:\", mean_dict)\n",
    "    print(\"STD:\", std_dict)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "dynamix_data = normalize_data_dynamix(dynamix_data)\n",
    "pred_data = normalize_data_critiq(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model for predictions\n",
    "from typing import Optional, Any, Dict, Tuple\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from control_dropping_rpal.RL.Networks.ExtractorNetworks import (\n",
    "    ObjectTactileEncoder_Additive,\n",
    "    TemporalObjectTactileEncoder_Additive,\n",
    ")\n",
    "\n",
    "DYNAMIX_OUTPUT_SIZES_DICT = {\n",
    "    \"finger_1_location\": 27,\n",
    "    \"finger_2_location\": 27,\n",
    "    \"finger_3_location\": 27,\n",
    "    \"palm_location\": 27,\n",
    "    \"finger_1_tactile\": 9,\n",
    "    \"finger_2_tactile\": 9,\n",
    "    \"finger_3_tactile\": 9,\n",
    "    \"palm_tactile\": 9,\n",
    "    \"obj_location\": 42,\n",
    "    \"hand_config\": 7,\n",
    "    \"obj_count\": 5,\n",
    "    \"progress_bar\": 1,\n",
    "    \"reward\": 1,\n",
    "}\n",
    "\n",
    "PRED_OUTPUT_SIZES_DICT = {\n",
    "    \"action\": 5,\n",
    "    \"reward\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "class DynamixModel(nn.Module):\n",
    "    \"\"\"Given (State, Action): Predict (N_state)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim_high=1024,\n",
    "        embed_dim_low=256,\n",
    "        device=\"cuda\",\n",
    "        dropout_prob=0.05,\n",
    "        num_tsf_layer=4,\n",
    "        num_residual_blocks=4,\n",
    "        vec_encoding_size=8,\n",
    "        use_mask=False,\n",
    "        encoder: Optional[TemporalObjectTactileEncoder_Additive] = None,\n",
    "    ):\n",
    "        super(DynamixModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.object_encoder = (\n",
    "            encoder\n",
    "            if encoder\n",
    "            else TemporalObjectTactileEncoder_Additive(\n",
    "                observation_space=state_space,\n",
    "                vec_encoding_size=vec_encoding_size,\n",
    "                t_dim_size=T_buffer,\n",
    "                load_pretrain=False,\n",
    "                num_tsf_layer=num_tsf_layer,\n",
    "                use_mask=use_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.act_size = 5\n",
    "        self.cat_size = self.object_encoder.flatten_size + 5  # action shape\n",
    "        self.embed_dim_high = embed_dim_high\n",
    "        self.embed_dim_low = embed_dim_low\n",
    "        self.activation = nn.GELU\n",
    "        self.join_keys = [\"action\"]\n",
    "\n",
    "        self.delta_state_network = nn.Sequential(\n",
    "            # CAST\n",
    "            ResidualBlocks1D(\n",
    "                feature_dim=vec_encoding_size + 5,\n",
    "                num_blocks=num_residual_blocks,\n",
    "                embed_dim=embed_dim_high,\n",
    "            ),\n",
    "            # GATING\n",
    "            nn.Linear(vec_encoding_size + 5, vec_encoding_size),\n",
    "        )\n",
    "\n",
    "        modules = {}\n",
    "        for k, size in DYNAMIX_OUTPUT_SIZES_DICT.items():\n",
    "            modules[k] = nn.Sequential(\n",
    "                nn.Linear(vec_encoding_size, self.embed_dim_high),\n",
    "                self.activation(),\n",
    "                nn.Dropout(p=dropout_prob),\n",
    "                nn.Linear(self.embed_dim_high, self.embed_dim_low),\n",
    "                self.activation(),\n",
    "                nn.Dropout(p=dropout_prob),\n",
    "                nn.Linear(self.embed_dim_low, size),\n",
    "            )\n",
    "\n",
    "        self.networks = nn.ModuleDict(modules)\n",
    "\n",
    "\n",
    "    def forward(self, obs):\n",
    "        tac_encoding = self.object_encoder(obs)\n",
    "        tac_encoding = tac_encoding.view((tac_encoding.shape[0], -1))\n",
    "        cat_tensor = th.concatenate([tac_encoding, obs[\"action\"]], dim=1).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        state_delta = self.delta_state_network(cat_tensor)\n",
    "\n",
    "        # Intuition:\n",
    "        # Or critiq model is trying to find a relationship between one state embedding and another state embedding to produce an action and a reward\n",
    "        # Here we are trying to find the delta between our original state and the new state\n",
    "        # caused by the action such that we can predict it\n",
    "        embed_tnsor = state_delta + tac_encoding\n",
    "\n",
    "        # Benifets: I can take a state at T[0] and pass S[0] through our network to\n",
    "        # obtain an embedding e_0\n",
    "        # With e_0 and A_0 I can obtain e_1: use e_0 CONCAT A_0 with delta state network\n",
    "        # with e_1, I can obtain e_2 using A_1 via a similar process.\n",
    "        # Hence, this modeling scheme is powerful if future trajectories can be reliably obtained in the embedding space of S\n",
    "\n",
    "        # TODO: Test Utitlity\n",
    "        # embed_tnsor = F.normalize(F.tanh(state_delta) + F.sigmoid(tac_encoding), eps=1e-7) #TODO: Test Utitlity\n",
    "        # embed_tnsor = F.tanh((state_delta + tac_encoding ) * (1 / self.object_encoder.vec_encoding_size**0.5))\n",
    "        # embed_tnsor = state_delta\n",
    "\n",
    "        logit_map = {k: net(embed_tnsor) for k, net in self.networks.items()}\n",
    "        return logit_map\n",
    "\n",
    "    def load_checkpoint(self, path=None):\n",
    "        if path is None:\n",
    "            path = os.path.join(self.object_encoder.save_path, \"dynamix.pt\")\n",
    "\n",
    "        checkpoint = th.load(path, map_location=self.device)\n",
    "        self.object_encoder.load_state_dict(checkpoint[\"object_encoder\"])\n",
    "        self.delta_state_network.load_state_dict(checkpoint[\"delta_state_network\"])\n",
    "        self.networks.load_state_dict(checkpoint[\"networks\"])\n",
    "\n",
    "    def save_checkpoint(self, file=None):\n",
    "        print(\"[DynamixModel] Saving Checkpoint...\")\n",
    "        if file is None:\n",
    "            file = os.path.join(self.object_encoder.save_path, \"dynamix.pt\")\n",
    "\n",
    "        save_dict = {\n",
    "            \"object_encoder\": self.object_encoder.state_dict(),\n",
    "            \"delta_state_network\": self.delta_state_network.state_dict(),\n",
    "            \"networks\": self.networks.state_dict(),\n",
    "        }\n",
    "        th.save(save_dict, file)\n",
    "\n",
    "\n",
    "class CritiqModel(nn.Module):\n",
    "    \"\"\"Given (State, N_state): Predict (Action, Reward)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim_high=1024,\n",
    "        embed_dim_low=256,\n",
    "        device=\"cuda\",\n",
    "        dropout_prob=0.05,\n",
    "        num_tsf_layer=4,\n",
    "        num_residual_blocks=4,\n",
    "        vec_encoding_size=8,\n",
    "        use_mask=False,\n",
    "        encoder: Optional[TemporalObjectTactileEncoder_Additive] = None,\n",
    "    ):\n",
    "        super(CritiqModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.object_encoder = (\n",
    "            encoder\n",
    "            if encoder\n",
    "            else TemporalObjectTactileEncoder_Additive(\n",
    "                observation_space=state_space,\n",
    "                vec_encoding_size=vec_encoding_size,\n",
    "                t_dim_size=T_buffer,\n",
    "                load_pretrain=False,\n",
    "                num_tsf_layer=num_tsf_layer,\n",
    "                use_mask=use_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.act_size = 5\n",
    "        self.cat_size = self.object_encoder.flatten_size + 5  # action shape\n",
    "        self.embed_dim_high = embed_dim_high\n",
    "        self.embed_dim_low = embed_dim_low\n",
    "        self.activation = nn.GELU\n",
    "        self.join_keys = [\"action\"]\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            # Recieves Delta State\n",
    "            ResidualBlocks1D(\n",
    "                feature_dim=vec_encoding_size * 2,\n",
    "                num_blocks=5,\n",
    "                embed_dim=embed_dim_high,\n",
    "            ),\n",
    "            nn.Linear(vec_encoding_size * 2, vec_encoding_size),\n",
    "            self.activation(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "\n",
    "        # Time estimation:\n",
    "        self.time_estimator = nn.Sequential(\n",
    "            # Recieves Delta State\n",
    "            nn.Linear(vec_encoding_size, vec_encoding_size),\n",
    "            self.activation(),\n",
    "            nn.Linear(vec_encoding_size, 1),\n",
    "        )\n",
    "\n",
    "        modules = {}\n",
    "        for k, size in PRED_OUTPUT_SIZES_DICT.items():\n",
    "            modules[k] = nn.Sequential(\n",
    "                nn.Linear(vec_encoding_size, self.embed_dim_high),\n",
    "                self.activation(),\n",
    "                nn.Dropout(p=dropout_prob),\n",
    "                nn.Linear(self.embed_dim_high, self.embed_dim_low),\n",
    "                self.activation(),\n",
    "                nn.Dropout(p=dropout_prob),\n",
    "                nn.Linear(self.embed_dim_low, size),\n",
    "            )\n",
    "\n",
    "        self.networks = nn.ModuleDict(modules)\n",
    "\n",
    "    def forward(self, state, n_state):\n",
    "        # Feed forward S[T] and S[T+1]\n",
    "        embed_state = self.object_encoder(state)\n",
    "        n_embed_state = self.object_encoder(n_state)\n",
    "        embed_state = embed_state.view((embed_state.shape[0], -1))\n",
    "        n_embed_state = n_embed_state.view((n_embed_state.shape[0], -1))\n",
    "\n",
    "        # We obtain a delta\n",
    "        delta_state = n_embed_state - embed_state\n",
    "        # delta_estimation = (1 / self.time_estimator(embed_state)) * self.predictor(delta_state)\n",
    "        delta_concat = th.cat([embed_state, delta_state], dim=1).to(self.device)\n",
    "\n",
    "        delta_estimation = self.predictor(delta_concat)\n",
    "\n",
    "        logit_map = {k: net(delta_estimation) for k, net in self.networks.items()}\n",
    "\n",
    "        return logit_map\n",
    "\n",
    "    def load_checkpoint(self, path=None):\n",
    "        if path is None:\n",
    "            path = os.path.join(self.object_encoder.save_path, \"critiq.pt\")\n",
    "\n",
    "        checkpoint = th.load(path, map_location=self.device)\n",
    "        self.object_encoder.load_state_dict(checkpoint[\"object_encoder\"])\n",
    "        self.predictor.load_state_dict(checkpoint[\"predictor\"])\n",
    "        self.time_estimator.load_state_dict(checkpoint[\"time_estimator\"])\n",
    "        self.networks.load_state_dict(checkpoint[\"networks\"])\n",
    "\n",
    "    def save_checkpoint(self, file=None):\n",
    "        print(\"[CritiqModel] Saving Checkpoint...\")\n",
    "        if file is None:\n",
    "            file = os.path.join(self.object_encoder.save_path, \"critiq.pt\")\n",
    "\n",
    "        save_dict = {\n",
    "            \"object_encoder\": self.object_encoder.state_dict(),\n",
    "            \"predictor\": self.predictor.state_dict(),\n",
    "            \"time_estimator\": self.time_estimator.state_dict(),\n",
    "            \"networks\": self.networks.state_dict(),\n",
    "        }\n",
    "        th.save(save_dict, file)\n",
    "\n",
    "\n",
    "def make_dynamix_and_predictor(\n",
    "    model_args: Dict[str, Any]\n",
    ") -> Tuple[DynamixModel, CritiqModel]:\n",
    "    # Extract common arguments\n",
    "    embed_dim_high = model_args.get(\"embed_dim_high\", 1024)\n",
    "    embed_dim_low = model_args.get(\"embed_dim_low\", 256)\n",
    "    device = model_args.get(\"device\", \"cuda\")\n",
    "    dropout_prob = model_args.get(\"dropout_prob\", 0.05)\n",
    "    num_tsf_layer = model_args.get(\"num_tsf_layer\", 4)\n",
    "    num_residual_blocks = model_args.get(\"num_residual_blocks\", 4)\n",
    "    vec_encoding_size = model_args.get(\"vec_encoding_size\", 8)\n",
    "    use_mask = model_args.get(\"use_mask\", False)\n",
    "\n",
    "    # Create the shared encoder\n",
    "    encoder = TemporalObjectTactileEncoder_Additive(\n",
    "        observation_space=model_args.get(\"state_space\"),\n",
    "        vec_encoding_size=vec_encoding_size,\n",
    "        t_dim_size=model_args.get(\"T_buffer\"),\n",
    "        load_pretrain=False,\n",
    "        num_tsf_layer=num_tsf_layer,\n",
    "        use_mask=use_mask,\n",
    "    )\n",
    "\n",
    "    # Create DynamixModel\n",
    "    dynamix_model = DynamixModel(\n",
    "        embed_dim_high=embed_dim_high,\n",
    "        embed_dim_low=embed_dim_low,\n",
    "        device=device,\n",
    "        dropout_prob=dropout_prob,\n",
    "        num_tsf_layer=num_tsf_layer,\n",
    "        num_residual_blocks=num_residual_blocks,\n",
    "        vec_encoding_size=vec_encoding_size,\n",
    "        use_mask=use_mask,\n",
    "        encoder=encoder,\n",
    "    )\n",
    "\n",
    "    # Create CritiqModel\n",
    "    critiq_model = CritiqModel(\n",
    "        embed_dim_high=embed_dim_high,\n",
    "        embed_dim_low=embed_dim_low,\n",
    "        device=device,\n",
    "        dropout_prob=dropout_prob,\n",
    "        num_tsf_layer=num_tsf_layer,\n",
    "        num_residual_blocks=num_residual_blocks,\n",
    "        vec_encoding_size=vec_encoding_size,\n",
    "        use_mask=use_mask,\n",
    "        encoder=encoder,\n",
    "    )\n",
    "\n",
    "    return dynamix_model, critiq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Collection for States/Preds\n",
    "import time as t\n",
    "\n",
    "env = BerrettHandGym(detailed_training=True, is_val=True)\n",
    "\n",
    "sim = env.simController\n",
    "\n",
    "# RPAL\n",
    "MAX_ACTION_COEF = 2.5\n",
    "\n",
    "\n",
    "def _get_sampled_action():\n",
    "    action_coef = np.random.uniform() * MAX_ACTION_COEF\n",
    "    action = (\n",
    "        action_coef * env.action_space.sample()\n",
    "        if np.random.uniform() > 0.5\n",
    "        else np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    )\n",
    "    return action\n",
    "\n",
    "\n",
    "def DataCollect(num_steps):\n",
    "    done = False\n",
    "    data = []\n",
    "    data_point = env.reset()[0].copy()\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        try:\n",
    "            if done:\n",
    "                save_data(\n",
    "                    data,\n",
    "                    os.path.join(\n",
    "                        CONTROL_DROP_DIR,\n",
    "                        \"Data_Collection\",\n",
    "                        \"Time_Dependent_Samples_4\",\n",
    "                    ),\n",
    "                )\n",
    "                data = []\n",
    "                data_point = env.reset()[0].copy()\n",
    "            i += 1\n",
    "            action = _get_sampled_action()\n",
    "            # if model == None else model.predict(data_point,)\n",
    "            print(\"Action:\", action)\n",
    "            data_point[\"action\"] = action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # t.sleep(0.05)\n",
    "            pred_state = env.simController.get_pred_state()\n",
    "            pred_state[\"reward\"] = reward\n",
    "            print(\"Pred State:\", pred_state.keys())\n",
    "            data.append((data_point, pred_state))\n",
    "            data_point = state\n",
    "        except:\n",
    "            data = []\n",
    "            data_point = env.reset()[0].copy()\n",
    "\n",
    "\n",
    "DataCollect(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Collection for States/Preds\n",
    "import time as t\n",
    "\n",
    "env = BerrettHandGym(detailed_training=True, is_val=True)\n",
    "sim = env.simController\n",
    "\n",
    "# RPAL\n",
    "MAX_ACTION_COEF = 2.5\n",
    "\n",
    "\n",
    "def _get_sampled_action():\n",
    "    action_coef = np.random.uniform() * MAX_ACTION_COEF\n",
    "    action = (\n",
    "        action_coef * env.action_space.sample()\n",
    "        if np.random.uniform() > 0.5\n",
    "        else np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    )\n",
    "    return action\n",
    "\n",
    "\n",
    "def DataCollect(num_steps):\n",
    "    done = False\n",
    "    data = []\n",
    "    data_point = env.reset()[0].copy()\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        try:\n",
    "            if done:\n",
    "                save_data(\n",
    "                    data,\n",
    "                    os.path.join(\n",
    "                        CONTROL_DROP_DIR,\n",
    "                        \"Data_Collection\",\n",
    "                        \"Action_Pred_Time_Dependent_Samples_4\",\n",
    "                    ),\n",
    "                )\n",
    "                data = []\n",
    "                data_point = env.reset()[0].copy()\n",
    "            i += 1\n",
    "            action = _get_sampled_action()\n",
    "            # if model == None else model.predict(data_point,)\n",
    "            print(\"Action:\", action)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # t.sleep(0.05)\n",
    "            pred_state = state.copy()\n",
    "            pred_state[\"reward\"] = reward\n",
    "            pred_state[\"action\"] = action\n",
    "            print(\"Pred State:\", pred_state.keys())\n",
    "            data.append((data_point, pred_state))\n",
    "            data_point = state\n",
    "        except:\n",
    "            data = []\n",
    "            data_point = env.reset()[0].copy()\n",
    "\n",
    "\n",
    "DataCollect(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized simulation data collection:\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "from control_dropping_rpal.Utils.env_utils import AsyncVectorEnv\n",
    "from control_dropping_rpal.RL.control_dropping_env import BerretHandGymRayLibWrapper\n",
    "import time\n",
    "\n",
    "env = AsyncVectorEnv(\n",
    "    lambda: BerretHandGymRayLibWrapper(\n",
    "        {\n",
    "            \"test\": False,\n",
    "            \"cluster_index\": 3,\n",
    "            \"sim_port\": None,\n",
    "            \"object_type\": \"Sphere\",\n",
    "            \"object_quantity\": 7,\n",
    "            \"detailed_training\": False,\n",
    "            \"detailed_save_dir\": None,\n",
    "            \"plot_params\": [\"history\", \"avg_ke\", \"avg_vel\", \"avg_rew\"],\n",
    "            \"is_val\": False,\n",
    "        }\n",
    "    ),\n",
    "    num_envs=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "\n",
    "def DataCollectDynamix(env: AsyncVectorEnv, num_steps: int):\n",
    "    data = {i: [] for i in range(env.num_envs)}\n",
    "    data_points, _, _, _, _ = env.poll()\n",
    "    data_points = data_points.copy()\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        while len(data_points.keys()) == 0:  # All environments are currently resetting:\n",
    "            time.sleep(0.05)\n",
    "            data_points, _, _, _, _ = env.poll()\n",
    "            data_points = data_points.copy()\n",
    "\n",
    "        actions = {}\n",
    "        for env_id in data_points.keys():\n",
    "            action = _get_sampled_action(env.action_spaces[env_id])\n",
    "            actions[env_id] = action\n",
    "            data_points[env_id][\"action\"] = action\n",
    "\n",
    "        env.send_actions(actions)\n",
    "        new_obs, rewards, dones, _, _ = env.poll()\n",
    "        new_obs = new_obs.copy()\n",
    "\n",
    "        for env_id in new_obs:\n",
    "            if env_id in data_points:\n",
    "                pred_state = env.envs[env_id].agent.simController.get_pred_state()\n",
    "                pred_state[\"reward\"] = rewards[env_id]\n",
    "                data[env_id].append((data_points[env_id], pred_state))\n",
    "\n",
    "                if dones[env_id]:\n",
    "                    save_data(\n",
    "                        data[env_id],\n",
    "                        os.path.join(\n",
    "                            CONTROL_DROP_DIR,\n",
    "                            \"Data_Collection\",\n",
    "                            \"Time_Dependent_Samples_3\",  # Controls the path of where the collected data gets stored to, change this for new experiments\n",
    "                        ),\n",
    "                    )\n",
    "                    data[env_id] = []\n",
    "\n",
    "        data_points = new_obs.copy()\n",
    "\n",
    "        print(f\"Step {i+1}/{num_steps}\")\n",
    "\n",
    "\n",
    "def DataCollectCritiq(env: AsyncVectorEnv, num_steps: int):\n",
    "    data = {i: [] for i in range(env.num_envs)}\n",
    "    data_points, _, _, _, _ = env.poll()\n",
    "    data_points = data_points.copy()\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        while len(data_points.keys()) == 0:  # All environments are currently resetting:\n",
    "            time.sleep(0.05)\n",
    "            data_points, _, _, _, _ = env.poll()\n",
    "            data_points = data_points.copy()\n",
    "\n",
    "        actions = {}\n",
    "        for env_id in data_points.keys():\n",
    "            action = _get_sampled_action(env.action_spaces[env_id])\n",
    "            actions[env_id] = action\n",
    "            data_points[env_id][\"action\"] = action\n",
    "\n",
    "        env.send_actions(actions)\n",
    "        new_obs, rewards, dones, _, _ = env.poll()\n",
    "        new_obs = new_obs.copy()\n",
    "\n",
    "        for env_id in new_obs:\n",
    "            if env_id in data_points:\n",
    "                pred_state = new_obs[env_id]\n",
    "                pred_state[\"reward\"] = rewards[env_id]\n",
    "                pred_state[\"action\"] = actions[env_id]\n",
    "                data[env_id].append((data_points[env_id], pred_state))\n",
    "\n",
    "                if dones[env_id]:\n",
    "                    save_data(\n",
    "                        data[env_id],\n",
    "                        os.path.join(\n",
    "                            CONTROL_DROP_DIR,\n",
    "                            \"Data_Collection\",\n",
    "                            \"Action_Pred_Time_Dependent_Samples_1\",  # Controls the path of where the collected data gets stored to, change this for new experiments\n",
    "                        ),\n",
    "                    )\n",
    "                    data[env_id] = []\n",
    "\n",
    "        data_points = new_obs.copy()\n",
    "\n",
    "        print(f\"Step {i+1}/{num_steps}\")\n",
    "\n",
    "\n",
    "def _get_sampled_action(action_space):\n",
    "    MAX_ACTION_COEF = 10.0\n",
    "    action_coef = np.random.uniform() * MAX_ACTION_COEF\n",
    "    action = (\n",
    "        action_coef * action_space.sample()\n",
    "        if np.random.uniform() > 0.5\n",
    "        else np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    )\n",
    "    return action\n",
    "\n",
    "\n",
    "# DataCollectDynamix(env, 50000)\n",
    "DataCollectCritiq(env, 100000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Collection for States/Preds\n",
    "import time as t\n",
    "\n",
    "sim = env.simController\n",
    "\n",
    "# RPAL\n",
    "MAX_ACTION_COEF = 10.0\n",
    "\n",
    "\n",
    "def _get_sampled_action():\n",
    "    action_coef = np.random.uniform() * MAX_ACTION_COEF\n",
    "    action = (\n",
    "        action_coef * env.action_space.sample()\n",
    "        if np.random.uniform() > 0.5\n",
    "        else np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    )\n",
    "    return action\n",
    "\n",
    "\n",
    "def DataCollect(num_steps):\n",
    "    done = False\n",
    "    data = []\n",
    "    data_point = env.reset()[0].copy()\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        try:\n",
    "            if done:\n",
    "                save_data(\n",
    "                    data,\n",
    "                    os.path.join(\n",
    "                        CONTROL_DROP_DIR,\n",
    "                        \"Data_Collection\",\n",
    "                        \"Action_Pred_Time_Dependent_Samples_1\",\n",
    "                    ),\n",
    "                )\n",
    "                break\n",
    "                data = []\n",
    "                data_point = env.reset()[0].copy()\n",
    "            i += 1\n",
    "            action = _get_sampled_action()\n",
    "            # if model == None else model.predict(data_point,)\n",
    "            print(\"Action:\", action)\n",
    "            data_point[\"action\"] = action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # t.sleep(0.05)\n",
    "            # pred_state = env.simController.get_pred_state()\n",
    "            state[\"reward\"] = reward\n",
    "            print(\"Pred State:\", state.keys())\n",
    "            data.append((data_point, state))\n",
    "            data_point = state\n",
    "        except:\n",
    "            data = []\n",
    "            data_point = env.reset()[0].copy()\n",
    "\n",
    "    m_data = load_data_state_format(\n",
    "        os.path.join(\n",
    "            CONTROL_DROP_DIR,\n",
    "            \"Data_Collection\",\n",
    "            \"Action_Pred_Time_Dependent_Samples_1\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(data, m_data)\n",
    "\n",
    "\n",
    "DataCollect(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augments Data into Correct Format (Converts the Dict objects from Key, Idx, value to Idx, Key, value format)\n",
    "\n",
    "\n",
    "def convert_dataset_dynamix(\n",
    "    data,\n",
    "):\n",
    "    num_samples = len(data[0][\"palm_location\"])\n",
    "    state_elements = {\n",
    "        \"palm_tactile\": [],\n",
    "        \"finger_1_tactile\": [],\n",
    "        \"finger_2_tactile\": [],\n",
    "        \"finger_3_tactile\": [],\n",
    "        \"palm_location\": [],\n",
    "        \"finger_1_location\": [],\n",
    "        \"finger_2_location\": [],\n",
    "        \"finger_3_location\": [],\n",
    "        \"obj_location\": [],\n",
    "        \"obj_velocity\": [],\n",
    "        \"action\": [],\n",
    "        \"state_attrib\": [],\n",
    "    }\n",
    "    pred_elements = {\n",
    "        \"palm_tactile\": [],\n",
    "        \"finger_1_tactile\": [],\n",
    "        \"finger_2_tactile\": [],\n",
    "        \"finger_3_tactile\": [],\n",
    "        \"palm_location\": [],\n",
    "        \"finger_1_location\": [],\n",
    "        \"finger_2_location\": [],\n",
    "        \"finger_3_location\": [],\n",
    "        \"obj_location\": [],\n",
    "        \"obj_count\": [],\n",
    "        \"reward\": [],\n",
    "        \"progress_bar\": [],\n",
    "    }\n",
    "\n",
    "    state = data[0]\n",
    "    prediction = data[1]\n",
    "    for i in range(num_samples):\n",
    "        for k in state_elements.keys():\n",
    "            state_elements[k].append(\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(state[k][i], dtype=th.float32),\n",
    "                    nan=0.0,\n",
    "                    posinf=0.0,\n",
    "                    neginf=0.0,\n",
    "                )\n",
    "            )\n",
    "        for k in pred_elements.keys():\n",
    "            pred_elements[k].append(\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(\n",
    "                        np.array(\n",
    "                            prediction[k][i],\n",
    "                        ).flatten(),\n",
    "                        dtype=th.float32,\n",
    "                    ),\n",
    "                    nan=0.0,\n",
    "                    posinf=0.0,\n",
    "                    neginf=0.0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return state_elements, pred_elements\n",
    "\n",
    "\n",
    "def convert_dataset_critiq(\n",
    "    data,\n",
    "):\n",
    "    num_samples = len(data[0][\"palm_location\"])\n",
    "    state_elements = {\n",
    "        \"palm_tactile\": [],\n",
    "        \"finger_1_tactile\": [],\n",
    "        \"finger_2_tactile\": [],\n",
    "        \"finger_3_tactile\": [],\n",
    "        \"palm_location\": [],\n",
    "        \"finger_1_location\": [],\n",
    "        \"finger_2_location\": [],\n",
    "        \"finger_3_location\": [],\n",
    "        \"obj_location\": [],\n",
    "        \"obj_velocity\": [],\n",
    "        \"state_attrib\": [],\n",
    "    }\n",
    "\n",
    "    n_state_elements = {\n",
    "        \"palm_tactile\": [],\n",
    "        \"finger_1_tactile\": [],\n",
    "        \"finger_2_tactile\": [],\n",
    "        \"finger_3_tactile\": [],\n",
    "        \"palm_location\": [],\n",
    "        \"finger_1_location\": [],\n",
    "        \"finger_2_location\": [],\n",
    "        \"finger_3_location\": [],\n",
    "        \"obj_location\": [],\n",
    "        \"obj_velocity\": [],\n",
    "        \"state_attrib\": [],\n",
    "    }\n",
    "\n",
    "    pred_elements = {\n",
    "        \"reward\": [],\n",
    "        \"action\": [],\n",
    "    }\n",
    "\n",
    "    state = data[0]\n",
    "    n_state = data[1]\n",
    "    prediction = data[2]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for k in state_elements.keys():\n",
    "            state_elements[k].append(\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(state[k][i], dtype=th.float32),\n",
    "                    nan=0.0,\n",
    "                    posinf=0.0,\n",
    "                    neginf=0.0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for k in n_state_elements.keys():\n",
    "            n_state_elements[k].append(\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(n_state[k][i], dtype=th.float32),\n",
    "                    nan=0.0,\n",
    "                    posinf=0.0,\n",
    "                    neginf=0.0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for k in pred_elements.keys():\n",
    "            pred_elements[k].append(\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(\n",
    "                        np.array(\n",
    "                            prediction[k][i],\n",
    "                        ).flatten(),\n",
    "                        dtype=th.float32,\n",
    "                    ),\n",
    "                    nan=0.0,\n",
    "                    posinf=0.0,\n",
    "                    neginf=0.0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return state_elements, n_state_elements, pred_elements\n",
    "\n",
    "\n",
    "dynamix_data = convert_dataset_dynamix(dynamix_data)\n",
    "pred_data = convert_dataset_critiq(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Class for Object Motion Prediction\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "class ObjDataset(Dataset):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.elements, self.preds = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(self.elements[\"action\"])\n",
    "        self.rand_sort()\n",
    "\n",
    "    def rand_sort(self):\n",
    "        permutations = np.random.permutation(self.num_samples)\n",
    "        self.elements = {\n",
    "            k: [val[p] for p in permutations] for k, val in self.elements.items()\n",
    "        }\n",
    "        self.preds = {\n",
    "            k: [val[p] for p in permutations] for k, val in self.preds.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the corresponding elements for the given index\n",
    "        # y_output = self.y_outputs[idx: min(idx+BATCH_SIZE, self.num_samples)]\n",
    "        sample_elements = {\n",
    "            key: value[idx : min(idx + self.batch_size, self.num_samples - 1)]\n",
    "            for key, value in self.elements.items()\n",
    "        }  #\n",
    "        y_output = {\n",
    "            key: value[idx : min(idx + self.batch_size, self.num_samples - 1)]\n",
    "            for key, value in self.preds.items()\n",
    "        }  #\n",
    "        if isinstance(sample_elements[\"action\"], list):\n",
    "            sample_elements = {\n",
    "                key: th.stack(value, dim=0) for key, value in sample_elements.items()\n",
    "            }\n",
    "        if isinstance(y_output[\"reward\"], list):\n",
    "            y_output = {key: th.stack(value, dim=0) for key, value in y_output.items()}\n",
    "        return sample_elements, y_output\n",
    "\n",
    "\n",
    "class CritiqDataset(Dataset):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.states, self.n_states, self.preds = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(self.states[\"palm_location\"])\n",
    "        self.rand_sort()\n",
    "\n",
    "    def rand_sort(self):\n",
    "        permutations = np.random.permutation(self.num_samples)\n",
    "        self.states = {\n",
    "            k: [val[p] for p in permutations] for k, val in self.states.items()\n",
    "        }\n",
    "        self.preds = {\n",
    "            k: [val[p] for p in permutations] for k, val in self.preds.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the corresponding elements for the given index\n",
    "        # y_output = self.y_outputs[idx: min(idx+BATCH_SIZE, self.num_samples)]\n",
    "        state_batch = {\n",
    "            key: value[idx : min(idx + self.batch_size, self.num_samples - 1)]\n",
    "            for key, value in self.states.items()\n",
    "        }\n",
    "        n_state_batch = {\n",
    "            key: value[idx : min(idx + self.batch_size, self.num_samples - 1)]\n",
    "            for key, value in self.states.items()\n",
    "        }\n",
    "        pred_batch = {\n",
    "            key: value[idx : min(idx + self.batch_size, self.num_samples - 1)]\n",
    "            for key, value in self.preds.items()\n",
    "        }\n",
    "\n",
    "        # Ensure outputs are Tensor:\n",
    "        if isinstance(state_batch[\"palm_location\"], list):\n",
    "            state_batch = {\n",
    "                key: th.stack(value, dim=0) for key, value in state_batch.items()\n",
    "            }\n",
    "\n",
    "        if isinstance(n_state_batch[\"palm_location\"], list):\n",
    "            n_state_batch = {\n",
    "                key: th.stack(value, dim=0) for key, value in n_state_batch.items()\n",
    "            }\n",
    "\n",
    "        if isinstance(pred_batch[\"reward\"], list):\n",
    "            pred_batch = {\n",
    "                key: th.stack(value, dim=0) for key, value in pred_batch.items()\n",
    "            }\n",
    "\n",
    "        return state_batch, n_state_batch, pred_batch\n",
    "\n",
    "\n",
    "# # print(dataset)\n",
    "# dataset = ObjDataset(dynamix_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 0.001\n",
    "end_lr = 0.00005\n",
    "factor = 0.999\n",
    "\n",
    "\n",
    "def lr_schedule():\n",
    "    global start_lr, end_lr, factor\n",
    "    ret_lr = start_lr\n",
    "    start_lr *= factor\n",
    "    return ret_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataloader for Model Training\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate data and labels from the batch\n",
    "    datas, preds = zip(*batch)\n",
    "    datas = [obs_as_tensor(d, \"cuda\") for d in datas]\n",
    "    preds = [obs_as_tensor(d, \"cuda\") for d in preds]\n",
    "\n",
    "    return th.stack(datas, dim=0), th.stack(preds, dim=0)\n",
    "\n",
    "\n",
    "# # Calculate the split sizes\n",
    "# train_size = int(0.92 * len(dataset))\n",
    "# validation_size = len(dataset) - train_size\n",
    "# train_set = (\n",
    "#     {k: val[:train_size] for k, val in dataset.elements.items()},\n",
    "#     {k: val[:train_size] for k, val in dataset.preds.items()},\n",
    "# )\n",
    "# val_set = (\n",
    "#     {k: val[:-validation_size] for k, val in dataset.elements.items()},\n",
    "#     {k: val[:-validation_size] for k, val in dataset.preds.items()},\n",
    "# )\n",
    "\n",
    "# print(\"Val Set:\", val_set)\n",
    "\n",
    "dynamix_dataset = ObjDataset(dynamix_data, BATCH_SIZE)\n",
    "train_size = int(0.92 * len(dynamix_dataset))\n",
    "validation_size = len(dynamix_dataset) - train_size\n",
    "dynamix_train_dataset, dynamix_validation_dataset = random_split(\n",
    "    dynamix_dataset, [train_size, validation_size]\n",
    ")\n",
    "\n",
    "critiq_dataset = CritiqDataset(pred_data, BATCH_SIZE)\n",
    "train_size = int(0.9 * len(critiq_dataset))\n",
    "validation_size = len(critiq_dataset) - train_size\n",
    "critiq_train_dataset, critiq_validation_dataset = random_split(\n",
    "    critiq_dataset, [train_size, validation_size]\n",
    ")\n",
    "\n",
    "# train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, )#collate_fn=collate_fn)\n",
    "# validation_data_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, )#collate_fn=collate_fn)\n",
    "\n",
    "for train_dataset, val_dataset in [\n",
    "    (\n",
    "        dynamix_train_dataset,\n",
    "        dynamix_validation_dataset,\n",
    "    ),\n",
    "    (critiq_train_dataset, critiq_validation_dataset),\n",
    "]:\n",
    "    print(\"Training Data:\")\n",
    "    batch = dynamix_train_dataset[0]\n",
    "    x, y = batch\n",
    "    for k, v in x.items():\n",
    "        print(\n",
    "            k,\n",
    "            v.shape,\n",
    "            v.min(),\n",
    "            v.max(),\n",
    "        )\n",
    "    for k, v in y.items():\n",
    "        print(\n",
    "            k,\n",
    "            v.shape,\n",
    "            v.min(),\n",
    "            v.max(),\n",
    "        )\n",
    "    print()\n",
    "    print(\"Validation Data:\")\n",
    "    batch = dynamix_validation_dataset[0]\n",
    "    x, y = batch\n",
    "    for k, v in x.items():\n",
    "        print(k, v.shape)\n",
    "        print(\"---\\n\")\n",
    "    for k, v in y.items():\n",
    "        print(k, v.shape)\n",
    "        print(\"---\\n\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Training Data:\")\n",
    "    batch = critiq_train_dataset[0]\n",
    "    x, y, z = batch\n",
    "    for k, v in x.items():\n",
    "        print(\n",
    "            k,\n",
    "            v.shape,\n",
    "            v.min(),\n",
    "            v.max(),\n",
    "        )\n",
    "    for k, v in y.items():\n",
    "        print(\n",
    "            k,\n",
    "            v.shape,\n",
    "            v.min(),\n",
    "            v.max(),\n",
    "        )\n",
    "    for k, v in z.items():\n",
    "        print(\n",
    "            k,\n",
    "            v.shape,\n",
    "            v.min(),\n",
    "            v.max(),\n",
    "        )\n",
    "\n",
    "    print()\n",
    "    print(\"Validation Data:\")\n",
    "    batch = critiq_validation_dataset[0]\n",
    "    x, y, z = batch\n",
    "    for k, v in x.items():\n",
    "        print(k, v.shape)\n",
    "        print(\"---\\n\")\n",
    "    for k, v in y.items():\n",
    "        print(k, v.shape)\n",
    "        print(\"---\\n\")\n",
    "    for k, v in z.items():\n",
    "        print(k, v.shape)\n",
    "        print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Try Including/Excluding Finger Values (2)\n",
    "# Diff between loss (Mean/Sum) ()\n",
    "# Use huggingface model\n",
    "# Model Predictive Control: Accuracy (1)\n",
    "#\n",
    "key_losses_dynamix = {\n",
    "    \"palm_tactile\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"finger_1_tactile\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"finger_2_tactile\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"finger_3_tactile\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"palm_location\": lambda y_pred, y_target: torch.zeros_like(\n",
    "        F.mse_loss(y_pred, y_target)\n",
    "    ),  # F.mse_loss(y_pred, y_target),\n",
    "    \"finger_1_location\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"finger_2_location\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"finger_3_location\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"obj_location\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"obj_count\": lambda y_pred, y_target: F.cross_entropy(y_pred, y_target),\n",
    "    \"reward\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "}\n",
    "\n",
    "key_losses_critiq = {\n",
    "    \"action\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "    \"reward\": lambda y_pred, y_target: F.mse_loss(y_pred, y_target),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Object Transformer!!\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import os\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    dynamix_model,\n",
    "    critiq_model,\n",
    "    dynamix_train_loader,\n",
    "    dynamix_val_loader,\n",
    "    critiq_train_loader,\n",
    "    critiq_val_loader,\n",
    "    batch_size=16,\n",
    "    epochs=1000,\n",
    "    learning_rate=5e-3,\n",
    "    log_interval=50,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    is_lstm=False,\n",
    "    patience=10,\n",
    "    num_workers=4,\n",
    "):\n",
    "\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    use_data_parallel = False\n",
    "    # Check if multiple GPUs are available\n",
    "    if th.cuda.device_count() > 1:\n",
    "        print(f\"Using {th.cuda.device_count()} GPUs\")\n",
    "        dynamix_model = nn.DataParallel(dynamix_model)\n",
    "        critiq_model = nn.DataParallel(critiq_model)\n",
    "        use_data_parallel = True\n",
    "\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    initial_lr = 0.00001\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    def dynamix_worker(model, device, data_queue, optimizer, data_lock, train_losses):\n",
    "        while True:\n",
    "            try:\n",
    "                data, target = data_queue.get(timeout=1)\n",
    "                if not data:\n",
    "                    continue\n",
    "                data = {k: v.to(device).squeeze(dim=0) for k, v in data.items()}\n",
    "                target = {k: v.to(device).squeeze(dim=0) for k, v in target.items()}\n",
    "                y_target = target\n",
    "                with data_lock:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(data)\n",
    "                    loss = th.stack(\n",
    "                        [\n",
    "                            key_losses_dynamix[key](y_pred[key], y_target[key])\n",
    "                            for key in key_losses_dynamix.keys()\n",
    "                            if key in y_pred and key in y_target\n",
    "                        ]\n",
    "                    ).sum()\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_schedule()\n",
    "                    train_losses.append(loss.item())\n",
    "                print(\"Loss:\", loss.item())\n",
    "                data_queue.task_done()\n",
    "\n",
    "            except queue.Empty:\n",
    "                break\n",
    "\n",
    "    def critiq_worker(model, device, data_queue, optimizer, data_lock, train_losses):\n",
    "        while True:\n",
    "            try:\n",
    "                state, n_state, target = data_queue.get(timeout=1)\n",
    "                # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "                state = {k: v.to(device).squeeze(dim=0) for k, v in state.items()}\n",
    "                n_state = {k: v.to(device).squeeze(dim=0) for k, v in n_state.items()}\n",
    "                target = {k: v.to(device).squeeze(dim=0) for k, v in target.items()}\n",
    "                y_target = target\n",
    "\n",
    "                with data_lock:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(state, n_state)\n",
    "                    loss = th.stack(\n",
    "                        [\n",
    "                            key_losses_critiq[key](y_pred[key], y_target[key])\n",
    "                            for key in key_losses_critiq.keys()\n",
    "                            if key in y_pred and key in y_target\n",
    "                        ]\n",
    "                    ).sum()\n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_schedule()\n",
    "                    train_losses.append(loss.item())\n",
    "                print(\"Loss:\", loss.item())\n",
    "                data_queue.task_done()\n",
    "\n",
    "            except queue.Empty:\n",
    "                break\n",
    "\n",
    "    def train(\n",
    "        dynamix_model,\n",
    "        critiq_model,\n",
    "        device,\n",
    "        dynamix_train_loader,\n",
    "        critiq_train_loader,\n",
    "        dynamix_optimizer,\n",
    "        critiq_optimizer,\n",
    "        num_workers=4,\n",
    "        is_lstm=is_lstm,\n",
    "    ):\n",
    "        for model in (\n",
    "            dynamix_model,\n",
    "            critiq_model,\n",
    "        ):\n",
    "            model.train()\n",
    "            model.to(device)\n",
    "        total_loss = 0\n",
    "        i = 0\n",
    "\n",
    "        ####### Thread safe worker threads\n",
    "        # dynamix_data_queue = queue.Queue()                print(\"Loss:\", loss.item())\n",
    "        # data_lock = threading.Lock()\n",
    "\n",
    "        # for _ in range(num_workers // 2):\n",
    "        #     worker_thread = threading.Thread(\n",
    "        #         target=dynamix_worker,\n",
    "        #         args=(\n",
    "        #             dynamix_model,\n",
    "        #             device,\n",
    "        #             dynamix_data_queue,\n",
    "        #             dynamix_optimizer,\n",
    "        #             data_lock,\n",
    "        #             train_losses,\n",
    "        #         ),\n",
    "        #     )\n",
    "        #     worker_thread.daemon = True\n",
    "        #     workers.append(worker_thread)\n",
    "\n",
    "        # for _ in range(num_workers - (num_workers // 2)):\n",
    "        #     worker_thread = threading.Thread(\n",
    "        #         target=critiq_worker,\n",
    "        #         args=(\n",
    "        #             critiq_model,state = {k: v.to(device).squeeze(dim=0) for k, v in state.items()}\n",
    "                # n_state = {k: v.to(device).squeeze(dim=0) for k, v in n_state.items()}\n",
    "                # target = {k: v.to(device).squeeze(dim=0) for k, v in target.items()}\n",
    "        #             critiq_data_queue,\n",
    "        #             critiq_optimizer,\n",
    "        #             data_lock,\n",
    "        #             train_losses,\n",
    "        #         ),\n",
    "        #     )\n",
    "        #     worker_thread.daemon = True\n",
    "        #     workers.append(worker_thread)\n",
    "\n",
    "        # for idx in range(0, len(dynamix_train_loader), BATCH_SIZE):                print(\"Loss:\", loss.item())\n",
    "        # for worker in workers:\n",
    "        #     worker.start()\n",
    "\n",
    "        # dynamix_data_queue.join()\n",
    "        # critiq_data_queue.join()\n",
    "\n",
    "        # for worker_thread in workers:\n",
    "        #     worker_thread.join()\n",
    "\n",
    "        # return total_loss / (\n",
    "        #     len(dynamix_train_loader.dataset) + len(critiq_train_loader.dataset)\n",
    "        # )\n",
    "        #######\n",
    "        for idx in range(0, min(len(dynamix_train_loader), len(critiq_train_loader)), BATCH_SIZE):\n",
    "            local_loss = 0\n",
    "            for optimizer in (dynamix_optimizer, critiq_optimizer):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            dynamix_data, dynamix_target = dynamix_train_loader[idx]\n",
    "            state, n_state, critiq_target = critiq_train_loader[idx]\n",
    "\n",
    "            dynamix_data = {k: v.to(device).squeeze(dim=0) for k, v in dynamix_data.items()}\n",
    "            y_target = {k: v.to(device).squeeze(dim=0) for k, v in dynamix_target.items()}\n",
    "            \n",
    "            y_pred =  dynamix_model(dynamix_data)      \n",
    "            loss = th.stack(\n",
    "                        [\n",
    "                            key_losses_dynamix[key](y_pred[key], y_target[key])\n",
    "                            for key in key_losses_dynamix.keys()\n",
    "                            if key in y_pred and key in y_target\n",
    "                        ]\n",
    "                    ).sum()   \n",
    "\n",
    "            loss.backward()\n",
    "            local_loss += loss.item()\n",
    "\n",
    "            state = {k: v.to(device).squeeze(dim=0) for k, v in state.items()}\n",
    "            n_state = {k: v.to(device).squeeze(dim=0) for k, v in n_state.items()}\n",
    "            y_target = {k: v.to(device).squeeze(dim=0) for k, v in critiq_target.items()}\n",
    "\n",
    "            y_pred = critiq_model(state, n_state)\n",
    "            \n",
    "            loss = th.stack(\n",
    "                [\n",
    "                    key_losses_critiq[key](y_pred[key], y_target[key])\n",
    "                    for key in key_losses_critiq.keys()\n",
    "                    if key in y_pred and key in y_target\n",
    "                ]\n",
    "            ).sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            local_loss += loss.item()\n",
    "            print(\"Loss:\", local_loss)\n",
    "\n",
    "            for model in (dynamix_model, critiq_model):\n",
    "                clip_grad_norm_(model.parameters(), 1.5)\n",
    "\n",
    "            for optimizer in (dynamix_optimizer, critiq_optimizer):\n",
    "                optimizer.step()\n",
    "\n",
    "            lr_schedule()\n",
    "            train_losses.append(local_loss)\n",
    "\n",
    "\n",
    "    def validation(\n",
    "        dynamix_model,\n",
    "        critiq_model,\n",
    "        device,\n",
    "        dynamix_val_loader,\n",
    "        critiq_val_loader,\n",
    "        is_lstm=is_lstm,\n",
    "    ):\n",
    "        for model in (\n",
    "            dynamix_model,\n",
    "            critiq_model,\n",
    "        ):\n",
    "            model.eval()\n",
    "\n",
    "        loss_total = 0\n",
    "        with th.no_grad():\n",
    "            for idx in range(0, len(dynamix_val_loader), BATCH_SIZE):\n",
    "                data, target = dynamix_val_loader[idx]\n",
    "                # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "                data = {k: v.to(device).squeeze(dim=0) for k, v in data.items()}\n",
    "                target = {k: v.to(device).squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "                y_target = target\n",
    "                y_pred = dynamix_model(data)\n",
    "\n",
    "                loss = th.stack(\n",
    "                    [\n",
    "                        key_losses_dynamix[key](y_pred[key], y_target[key])\n",
    "                        for key in key_losses_dynamix.keys() \n",
    "                        if key in y_pred and key in y_target\n",
    "                    ]\n",
    "                ).mean()  # + th.sum(weights_sq) / loss_total += loss.item()\n",
    "                loss_total += loss.item()\n",
    "\n",
    "            for idx in range(0, len(critiq_val_loader), BATCH_SIZE):\n",
    "                state, n_state, target = critiq_val_loader[idx]\n",
    "                # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "                state = {k: v.to(device).squeeze(dim=0) for k, v in state.items()}\n",
    "                n_state = {k: v.to(device).squeeze(dim=0) for k, v in n_state.items()}\n",
    "                target = {k: v.to(device).squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "                y_target = target\n",
    "                y_pred = critiq_model(state, n_state)\n",
    "\n",
    "                loss = th.stack(\n",
    "                    [\n",
    "                        key_losses_critiq[key](y_pred[key], y_target[key])\n",
    "                        for key in key_losses_critiq.keys() \n",
    "                        if key in y_pred and key in y_target\n",
    "                    ]\n",
    "                ).sum()  # + th.sum(weights_sq) / loss_total += loss.item()\n",
    "                loss_total += loss.item()\n",
    "\n",
    "        val_loss = loss_total / (\n",
    "            (len(dynamix_val_loader.dataset) + len(critiq_val_loader.dataset) // BATCH_SIZE)\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Validation_loss:\", val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    # params = []\n",
    "    for model in (\n",
    "        dynamix_model,\n",
    "        critiq_model,\n",
    "    ):\n",
    "        model.to(device)\n",
    "    #     params.extend(model.parameters())\n",
    "\n",
    "    dynamix_optimizer = optim.Adam(dynamix_model.parameters(), lr=initial_lr)\n",
    "    dynamix_scheduler = CosineAnnealingLR(\n",
    "        dynamix_optimizer, T_max=epochs, eta_min=0, verbose=True\n",
    "    )\n",
    "\n",
    "    critiq_optimizer = optim.Adam(critiq_model.parameters(), lr=initial_lr)\n",
    "    critiq_scheduler = CosineAnnealingLR(\n",
    "        critiq_optimizer, T_max=epochs, eta_min=0, verbose=True\n",
    "    )\n",
    "\n",
    "    warmup_epochs = 50\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_lr = initial_lr * (epoch / warmup_epochs)\n",
    "            for optimizer in (dynamix_optimizer, critiq_optimizer):\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = warmup_lr\n",
    "            train(\n",
    "                dynamix_model,\n",
    "                critiq_model,\n",
    "                device,\n",
    "                dynamix_train_loader,\n",
    "                critiq_train_loader,\n",
    "                dynamix_optimizer,\n",
    "                critiq_optimizer,\n",
    "                num_workers=num_workers,\n",
    "            )\n",
    "            validation(\n",
    "                dynamix_model,\n",
    "                critiq_model,\n",
    "                device,\n",
    "                dynamix_val_loader,\n",
    "                critiq_val_loader,\n",
    "            )\n",
    "        else:\n",
    "            dynamix_scheduler.step()\n",
    "            critiq_scheduler.step()\n",
    "            train_loss = train(\n",
    "                dynamix_model,\n",
    "                critiq_model,\n",
    "                device,\n",
    "                dynamix_train_loader,\n",
    "                critiq_train_loader,\n",
    "                dynamix_optimizer,\n",
    "                critiq_optimizer,\n",
    "            )\n",
    "            if epoch % 10 == 0:\n",
    "                val_loss = validation(\n",
    "                    dynamix_model,\n",
    "                    critiq_model,\n",
    "                    device,\n",
    "                    dynamix_val_loader,\n",
    "                    critiq_val_loader,\n",
    "                )\n",
    "            if epoch % 50 == 0:\n",
    "                (\n",
    "                    (\n",
    "                        model.module.save_checkpoint()\n",
    "                        if use_data_parallel\n",
    "                        else model.save_checkpoint()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "    plt.xlabel(\"Seq\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"train_loss.png\")\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(val_losses)), val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Seq\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(\"val_loss.png\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params and making the models:\n",
    "NUM_LAYERS_TRANSFORMER = 8\n",
    "NUM_RESIDUALS = 8\n",
    "EPOCHS = 1000\n",
    "VEC_ENCODING_SIZE = 512\n",
    "\n",
    "MODEL_ARGS = {\n",
    "    \"vec_encoding_size\": VEC_ENCODING_SIZE,\n",
    "    \"num_residuals\": NUM_RESIDUALS,\n",
    "    \"num_tsf_layer\": NUM_LAYERS_TRANSFORMER,\n",
    "    \"use_mask\": True,\n",
    "    \"dropout_prob\": 0.01,\n",
    "    \"embed_dim_low\": VEC_ENCODING_SIZE,\n",
    "    \"T_buffer\": T_buffer,\n",
    "    \"state_space\": state_space,\n",
    "}\n",
    "\n",
    "dynamix_model, critiq_model = make_dynamix_and_predictor(MODEL_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Warmup Steps! Stabilizing the learning.\n",
    "\n",
    "train_model(\n",
    "    log_interval=3,\n",
    "    learning_rate=0.0008,\n",
    "    dynamix_model=dynamix_model,\n",
    "    critiq_model=critiq_model,\n",
    "    epochs=EPOCHS,\n",
    "    dynamix_train_loader=dynamix_train_dataset,\n",
    "    dynamix_val_loader=dynamix_validation_dataset,\n",
    "    critiq_train_loader=critiq_train_dataset,\n",
    "    critiq_val_loader=critiq_validation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    is_lstm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamix_model.save_checkpoint(\n",
    "    f\"./temporal_dropping_state_predictor-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\"\n",
    ")\n",
    "\n",
    "critiq_model.save_checkpoint(\n",
    "    f\"./temporal_dropping_state_critic-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamix_model.load_checkpoint(\n",
    "    f\"./temporal_dropping_state_predictor-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\"\n",
    ")\n",
    "\n",
    "critiq_model.load_checkpoint(\n",
    "    f\"./temporal_dropping_state_critic-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\"\n",
    ")\n",
    "\n",
    "encoder = dynamix_model.object_encoder\n",
    "\n",
    "encoder.save_checkpoint(\n",
    "    \"./pretrained_object_encoder-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(train_data_loader))\n",
    "# x = {k: v.to('cuda').float() for k, v in x.items()}\n",
    "# y = y.to('cuda')\n",
    "# print((y - model(x)).norm() / y.shape[0])\n",
    "# model.load_checkpoint(f\"./dropping_state_predictor-{NUM_LAYERS_TRANSFORMER}_layers-{NUM_RESIDUALS}_residuals-{VEC_ENCODING_SIZE}-vecencoding_size-{EPOCHS}_epochs.pt\")\n",
    "# model.to(model.device)\n",
    "# model.object_encoder.use_mask = False\n",
    "model = dynamix_model\n",
    "model.to(model.device)\n",
    "model.eval()\n",
    "model.object_encoder.use_mask = False\n",
    "print(\"model:\", model.cat_size)\n",
    "acc = 0.0\n",
    "total = 0\n",
    "for i in range(0, len(dynamix_validation_dataset), BATCH_SIZE):\n",
    "    data, target = dynamix_validation_dataset[i]\n",
    "    # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "    data = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in data.items()}\n",
    "    target = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "    y_target = target\n",
    "    y_pred = model(data)\n",
    "\n",
    "    correct = th.argmax(y_target[\"obj_count\"], dim=1) == th.argmax(\n",
    "        F.softmax(y_pred[\"obj_count\"]), dim=1\n",
    "    )\n",
    "    acc += sum(correct.int()) / len(correct)\n",
    "    total += 1\n",
    "\n",
    "print(acc / total)\n",
    "\n",
    "data, target = dynamix_validation_dataset[0]\n",
    "# if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "data = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in data.items()}\n",
    "target = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "y_target = target\n",
    "y_pred = model(data)\n",
    "\n",
    "# Percent Error:\n",
    "print(((y_target[\"reward\"] - y_pred[\"reward\"]) / y_target[\"reward\"]).abs().mean())\n",
    "print(\n",
    "    ((y_target[\"obj_location\"] - y_pred[\"obj_location\"]) / y_target[\"obj_location\"])\n",
    "    .abs()\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "## Gate_SigTanh: 98%, Gate_None: 96%,\n",
    "\n",
    "# print([(model(x), y) for i in range(len(y))])\n",
    "\n",
    "model = critiq_model\n",
    "model.to(model.device)\n",
    "model.eval()\n",
    "\n",
    "acc = 0.0\n",
    "total = 0\n",
    "for i in range(0, len(critiq_validation_dataset), BATCH_SIZE):\n",
    "    state, n_state, target = critiq_validation_dataset[i]\n",
    "    # if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "    state = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in state.items()}\n",
    "    n_state = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in n_state.items()}\n",
    "    target = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "    y_target = target\n",
    "    y_pred = model(state, n_state)\n",
    "\n",
    "    correct = ((y_target[\"action\"] - y_pred[\"action\"])**2).mean().item()\n",
    "    acc += correct\n",
    "    total += 1\n",
    "    \n",
    "print(\"MSE Actions:\" , acc / total)\n",
    "\n",
    "state, n_state, target = critiq_validation_dataset[0]\n",
    "# if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "state = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in state.items()}\n",
    "n_state = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in n_state.items()}\n",
    "target = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "y_target = target\n",
    "y_pred = model(state, n_state)\n",
    "\n",
    "# Percent Error:\n",
    "print(\"reward:\", ((y_target[\"reward\"] - y_pred[\"reward\"]) / y_target[\"reward\"]).abs().mean())\n",
    "print(\"MSE Action:\", \n",
    "    ((y_target[\"action\"] - y_pred[\"action\"])**2)\n",
    "    .mean()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output[0].detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "model.object_encoder.trns_encoder.layers._modules[\"5\"].self_attn._forward_hooks.clear()\n",
    "model.object_encoder.trns_encoder.layers._modules[\"5\"].self_attn.register_forward_hook(\n",
    "    get_activation(\"last_layer_activation\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = validation_dataset[1]\n",
    "# if is_lstm: model.reset_hidden_state(data.shape[0])\n",
    "data = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in data.items()}\n",
    "target = {k: v.to(\"cuda\").squeeze(dim=0) for k, v in target.items()}\n",
    "\n",
    "y_target = target\n",
    "y_pred = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract the activations for the last layer\n",
    "last_layer_activations = activation[\"last_layer_activation\"]\n",
    "activations = last_layer_activations.cpu()\n",
    "# Assuming your activations are in shape (batch_size, encoding_vector_size)\n",
    "# You might need to adjust this depending on the actual shape of your activations\n",
    "batch_size, num_objs, encoding_vector_size = last_layer_activations.shape\n",
    "# last_layer_activations = last_layer_activations.cpu().reshape(batch_size, -1)\n",
    "# Create a heatmap\n",
    "print(th.argmax(y_target[\"obj_count\"][2]))\n",
    "plt.figure(figsize=(12, 4))  # Adjust the figure size as needed\n",
    "sns.heatmap([last_layer_activations.cpu().numpy()[2, -1]], cmap=\"viridis\")\n",
    "plt.ylabel(\"Batch Index\")\n",
    "plt.xlabel(\"Encoding Vector Dimension\")\n",
    "plt.title(\"Heatmap of Last Layer Activations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    1, 2, figsize=(24, 6)\n",
    ")  # 24 width to accommodate both side by side, 6 height\n",
    "\n",
    "# Heatmap for index 2\n",
    "sns.heatmap(\n",
    "    activations[2, :].reshape(-1, activations.shape[-1]), ax=axs[0], cmap=\"viridis\"\n",
    ")\n",
    "axs[0].set_title(\"Heatmap of Last Layer Activations for Index 2\")\n",
    "axs[0].set_ylabel(\"Batch Index (2)\")\n",
    "axs[0].set_xlabel(\"Encoding Vector Dimension\")\n",
    "\n",
    "# Heatmap for index 3\n",
    "sns.heatmap(\n",
    "    activations[3, :].reshape(-1, activations.shape[-1]), ax=axs[1], cmap=\"viridis\"\n",
    ")\n",
    "axs[1].set_title(\"Heatmap of Last Layer Activations for Index 3\")\n",
    "axs[1].set_ylabel(\"Batch Index (3)\")\n",
    "axs[1].set_xlabel(\"Encoding Vector Dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "for i in range(activations.shape[0] - 4):\n",
    "    print(f\"Index: {i}:\")\n",
    "    print(\n",
    "        (\n",
    "            activations[i, :, -1].unsqueeze(dim=0)\n",
    "            @ activations[i + 1, :, -1].unsqueeze(dim=1)\n",
    "            / (\n",
    "                activations[i, :, -1].unsqueeze(dim=0).norm()\n",
    "                + activations[i + 1, :, -1].unsqueeze(dim=1).norm()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        (\n",
    "            activations[i, :, -1].unsqueeze(dim=0)\n",
    "            @ activations[i + 3, :, -1].unsqueeze(dim=1)\n",
    "            / (\n",
    "                activations[i, :, -1].unsqueeze(dim=0).norm()\n",
    "                + activations[i + 3, :, -1].unsqueeze(dim=1).norm()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(activations[i, :, -1].mean())\n",
    "\n",
    "activations[2, :, -1].unsqueeze(dim=0).shape, activations[3, :, -1].unsqueeze(\n",
    "    dim=1\n",
    ").shape\n",
    "\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Predictive Control\n",
    "model = DynamixModel()\n",
    "model.load_checkpoint(\"./save_embed.pt\")\n",
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "\n",
    "def sample_model_actions(\n",
    "    state: Dict[str, np.array],\n",
    "    k_num_actions: int = 128,\n",
    "    device: Union[th.device, str] = \"cuda\",\n",
    "):\n",
    "    global env, model\n",
    "\n",
    "    def sample_action():\n",
    "        action = env.action_space.sample()\n",
    "        return action / 3.5\n",
    "\n",
    "    # Normalization of observation\n",
    "    for key, obs in state.items():\n",
    "        arr = np.where(np.isnan(obs), 0, obs)\n",
    "        arr = np.where(arr < -(4**2), 0, arr)\n",
    "        arr = np.where(arr > 4**2, 0, arr)\n",
    "        if 1 in arr.shape and len(arr.shape) > 1:\n",
    "            arr = arr.squeeze()\n",
    "        state[key] = arr\n",
    "    state = {k: [val.copy()] * k_num_actions for k, val in state.items()}\n",
    "    # Samples k-1 actions, and adds the 'no' action as a base case to ensure our model can always use no action\n",
    "    state[\"action\"] = [np.array([0, 0, 0, 0, 0])] + [\n",
    "        sample_action() for _ in range(k_num_actions - 1)\n",
    "    ]\n",
    "\n",
    "    # Create the tensor for the model\n",
    "    state_tensors = {\n",
    "        k: th.stack(\n",
    "            [\n",
    "                th.nan_to_num(\n",
    "                    th.tensor(v, dtype=th.float32), nan=0.0, posinf=0.0, neginf=0.0\n",
    "                ).to(device)\n",
    "                for v in val\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        for k, val in state.items()\n",
    "    }\n",
    "\n",
    "    preds = model(state_tensors)\n",
    "\n",
    "    # Use metric for selecting an action. In this case we use rewards:\n",
    "    rewards = preds[\"reward\"].cpu().detach().numpy()\n",
    "    best_action_idx = np.argmax(rewards)\n",
    "\n",
    "    # Return the action we sampled at that index\n",
    "    return state[\"action\"][best_action_idx] * 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Collection for States/Preds\n",
    "import time as t\n",
    "\n",
    "sim = env.simController\n",
    "\n",
    "\n",
    "def ModelPredictiveControl(num_steps):\n",
    "    done = False\n",
    "    state = env.reset()[0].copy()\n",
    "    i = 0\n",
    "    while i < num_steps:\n",
    "        if done:\n",
    "            state = env.reset()[0].copy()\n",
    "        i += 1\n",
    "        action = sample_model_actions(state, 512)  # sample 512\n",
    "        print(\"Action:\", action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        t.sleep(0.25)\n",
    "\n",
    "\n",
    "ModelPredictiveControl(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_dropping_rpal.RL.control_dropping_env import BerrettHandGym\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "e = BerrettHandGym()\n",
    "\n",
    "check_env(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcment Learning with RayLib[RL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initialization:: Use env stbl3 or raylib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch as th\n",
    "from torch import multiprocessing\n",
    "from control_dropping_rpal.RL.Networks.ActorCriticNetwork import RayDroppingModel\n",
    "from control_dropping_rpal.RL.control_dropping_env import BerretHandGymRayLibWrapper, T_buffer, default_action_space\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "import gymnasium as gym\n",
    "from torchrl.envs import GymWrapper\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "th.set_default_device(\"cuda\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define constants\n",
    "DATA_SAVE_PATH = os.path.join(os.getcwd(), \"..\", \"Data_Collection\")\n",
    "NUM_LAYERS_TRANSFORMER = 8\n",
    "NUM_RESIDUALS = 4\n",
    "EPOCHS = 1000\n",
    "VEC_ENCODING_SIZE = 256\n",
    "NUM_WORKERS = 1\n",
    "ENVS_PER_WORKER = 1\n",
    "SAVE_INTERVAL = 3\n",
    "NUM_INTERACTIONS = 1000\n",
    "\n",
    "model_config = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize this \n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    th.device(0)\n",
    "    if th.cuda.is_available() and not is_fork\n",
    "    else th.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000\n",
    "\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "config = {}  # Add any configuration parameters needed for your environment\n",
    "base_env = BerretHandGymRayLibWrapper(config)\n",
    "\n",
    "# Wrap the environment with GymWrapper\n",
    "env = GymWrapper(base_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_dropping_rpal.RL.Networks.ActorCriticNetwork import ControlDroppingPolicy\n",
    "\n",
    "\n",
    "# Create the actor and critic modules\n",
    "actor_module = TensorDictModule(\n",
    "    ControlDroppingPolicy(model_config),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_params\", \"state_value\"],\n",
    ")\n",
    "\n",
    "# Assuming your action space is continuous (you might need to adjust this if it's discrete)\n",
    "actor = ProbabilisticActor(\n",
    "    module=actor_module,\n",
    "    spec=default_action_space,\n",
    "    in_keys=[\"action_params\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=torch.distributions.Normal,\n",
    ")\n",
    "\n",
    "critic = ValueOperator(\n",
    "    module=actor_module,\n",
    "    in_keys=[\"state_value\"],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stbl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
